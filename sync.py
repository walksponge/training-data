#!/usr/bin/env python3
"""
Intervals.icu â†’ GitHub/Local JSON Export
Exports training data for LLM access.
Supports both automated GitHub sync and manual local export.

Version 3.5.1 - HRV Outlier Filter
  - Add _is_valid_hrv() helper to filter sensor errors (10-250ms range)
  - Applied to: baselines (7d/28d), Recovery Index, persistence counts, summaries
  - Fixes false alarms from sensor glitches (e.g., 255ms Amazfit/Garmin errors)

Version 3.5.0 - Race Calendar & Race-Week Protocol
  - 90-day race calendar from Intervals.icu RACE_A/B/C event categories
  - Three-layer race awareness: calendar (D-90), taper onset (D-14 to D-8), race week (D-7 to D-0)
  - Race-week protocol: day-by-day load targets (% of CTL), zone guidance, purpose labels
  - TSB projection for race day using PMC decay with zero assumed load
  - Event duration classification (short/medium/long) from moving_time
  - Carb loading triggers (â‰¥90min events), opener scheduling (D-2), go/no-go checklist
  - RACE_B lighter taper (50-65% budget vs 40-55% for RACE_A)
  - Race-specific alerts integrated into main alerts array

Version 3.4.1 - KeyError fix, defensive .get(), anonymization improvements
Version 3.4.0 - Aggregate durability (7d/28d decoupling), dual-timeframe TID, capability namespace
Version 3.3.4 - Seiler TID classification, Treff PI, multi-sport TID, 7â†’3 zone mapping
Version 3.3.0 - Graduated alerts, history.json, notifications, smart fitness metrics, ACWR/monotony/strain
"""

import requests
import json
import os
import argparse
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import base64
import math
import statistics
from collections import defaultdict
from pathlib import Path


class IntervalsSync:
    """Sync Intervals.icu data to GitHub repository or local file"""
    
    INTERVALS_BASE_URL = "https://intervals.icu/api/v1"
    GITHUB_API_URL = "https://api.github.com"
    FTP_HISTORY_FILE = "ftp_history.json"
    HISTORY_FILE = "history.json"
    UPSTREAM_REPO = "CrankAddict/section-11"
    CHANGELOG_FILE = "changelog.json"
    VERSION = "3.5.1"

    # Sport family mapping for per-sport monotony calculation
    # Multi-sport athletes get inflated total monotony when cross-training
    # adds a consistent TSS floor across days. Per-sport monotony isolates
    # the actual load variation within each modality.
    SPORT_FAMILIES = {
        "Ride": "cycling",
        "VirtualRide": "cycling",
        "MountainBikeRide": "cycling",
        "GravelRide": "cycling",
        "EBikeRide": "cycling",
        "VirtualSki": "ski",
        "NordicSki": "ski",
        "Walk": "walk",
        "Hike": "walk",
        "Run": "run",
        "VirtualRun": "run",
        "TrailRun": "run",
        "Swim": "swim",
        "Rowing": "rowing",
        "WeightTraining": "strength",
        "Yoga": "other",
        "Workout": "other",
    }
    
    # Activity types that may contain location data in their name
    OUTDOOR_TYPES = {"Ride", "MountainBikeRide", "GravelRide", "EBikeRide",
                     "Run", "TrailRun", "NordicSki", "Walk", "Hike"}
    
    def __init__(self, athlete_id: str, intervals_api_key: str, github_token: str = None, 
                 github_repo: str = None, debug: bool = False):
        self.athlete_id = athlete_id
        self.intervals_auth = base64.b64encode(f"API_KEY:{intervals_api_key}".encode()).decode()
        self.github_token = github_token
        self.github_repo = github_repo
        self.debug = debug
        self.script_dir = Path(__file__).parent
    
    def _intervals_get(self, endpoint: str, params: Dict = None) -> Dict:
        """Fetch from Intervals.icu API"""
        if endpoint:
            url = f"{self.INTERVALS_BASE_URL}/athlete/{self.athlete_id}/{endpoint}"
        else:
            url = f"{self.INTERVALS_BASE_URL}/athlete/{self.athlete_id}"
        headers = {
            "Authorization": f"Basic {self.intervals_auth}",
            "Accept": "application/json"
        }
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        return response.json()
    
    def _fetch_today_wellness(self) -> Dict:
        """
        Fetch today's wellness data which contains:
        - CTL, ATL, rampRate (but these include planned workouts!)
        - sportInfo with eFTP, W', P-max (accurate live estimates)
        - VO2max, sleep quality/hours, etc.
        """
        today = datetime.now().strftime("%Y-%m-%d")
        try:
            data = self._intervals_get(f"wellness/{today}")
            return data
        except Exception as e:
            if self.debug:
                print(f"  Could not fetch today's wellness: {e}")
            return {}
    
    def _extract_power_model_from_wellness(self, wellness_data: Dict) -> Dict:
        """
        Extract eFTP, W', P-max from wellness.sportInfo.
        These are the accurate live estimates that match the Intervals.icu UI.
        """
        sport_info = wellness_data.get("sportInfo", [])
        
        # Find cycling sport info
        cycling_info = None
        for sport in sport_info:
            if sport.get("type") == "Ride":
                cycling_info = sport
                break
        
        if not cycling_info:
            return {
                "eftp": None,
                "w_prime": None,
                "w_prime_kj": None,
                "p_max": None,
                "source": "unavailable"
            }
        
        eftp = cycling_info.get("eftp")
        w_prime = cycling_info.get("wPrime")
        p_max = cycling_info.get("pMax")
        
        if self.debug and eftp:
            print(f"  eFTP: {round(eftp)}W, W': {round(w_prime) if w_prime else 'N/A'}J, P-max: {round(p_max) if p_max else 'N/A'}W")
        
        return {
            "eftp": round(eftp, 1) if eftp else None,
            "w_prime": round(w_prime) if w_prime else None,
            "w_prime_kj": round(w_prime / 1000, 1) if w_prime else None,
            "p_max": round(p_max) if p_max else None,
            "source": "wellness.sportInfo"
        }
    
    def _load_ftp_history(self) -> Dict[str, Dict[str, int]]:
        """
        Load FTP history from local JSON file.
        
        Returns dict with structure:
        {
            "indoor": {"2026-01-01": 270, "2026-02-01": 275},
            "outdoor": {"2026-01-01": 280, "2026-02-01": 287}
        }
        """
        ftp_history_path = self.script_dir / self.FTP_HISTORY_FILE
        
        if ftp_history_path.exists():
            try:
                with open(ftp_history_path, 'r') as f:
                    data = json.load(f)
                    # Handle legacy format (flat dict) -> convert to new format
                    if data and not ("indoor" in data or "outdoor" in data):
                        if self.debug:
                            print(f"  Converting legacy FTP history format...")
                        return {"indoor": {}, "outdoor": data}
                    return data
            except Exception as e:
                if self.debug:
                    print(f"  Could not load FTP history: {e}")
                return {"indoor": {}, "outdoor": {}}
        return {"indoor": {}, "outdoor": {}}
    
    def _save_ftp_history(self, history: Dict[str, Dict[str, int]], 
                          current_ftp_indoor: int, current_ftp_outdoor: int) -> Dict[str, Dict[str, int]]:
        """
        Save current FTPs to history file.
        Tracks indoor and outdoor FTP separately.
        Only adds entry if FTP changed from most recent entry.
        """
        today = datetime.now().strftime("%Y-%m-%d")
        
        # Ensure structure exists
        if "indoor" not in history:
            history["indoor"] = {}
        if "outdoor" not in history:
            history["outdoor"] = {}
        
        # Update indoor FTP if changed
        if current_ftp_indoor:
            indoor_history = history["indoor"]
            if indoor_history:
                sorted_dates = sorted(indoor_history.keys(), reverse=True)
                most_recent = indoor_history[sorted_dates[0]]
                if current_ftp_indoor != most_recent:
                    history["indoor"][today] = current_ftp_indoor
                    if self.debug:
                        print(f"  Indoor FTP changed: {most_recent} â†’ {current_ftp_indoor}")
            else:
                history["indoor"][today] = current_ftp_indoor
                if self.debug:
                    print(f"  Indoor FTP recorded: {current_ftp_indoor}")
        
        # Update outdoor FTP if changed
        if current_ftp_outdoor:
            outdoor_history = history["outdoor"]
            if outdoor_history:
                sorted_dates = sorted(outdoor_history.keys(), reverse=True)
                most_recent = outdoor_history[sorted_dates[0]]
                if current_ftp_outdoor != most_recent:
                    history["outdoor"][today] = current_ftp_outdoor
                    if self.debug:
                        print(f"  Outdoor FTP changed: {most_recent} â†’ {current_ftp_outdoor}")
            else:
                history["outdoor"][today] = current_ftp_outdoor
                if self.debug:
                    print(f"  Outdoor FTP recorded: {current_ftp_outdoor}")
        
        # Save to file
        ftp_history_path = self.script_dir / self.FTP_HISTORY_FILE
        try:
            with open(ftp_history_path, 'w') as f:
                json.dump(history, f, indent=2, sort_keys=True)
            if self.debug:
                print(f"  FTP history saved to {ftp_history_path}")
        except Exception as e:
            if self.debug:
                print(f"  Could not save FTP history: {e}")
        
        return history
    
    def _calculate_benchmark_index(self, current_ftp: int, ftp_history: Dict[str, int], 
                                    ftp_type: str = "indoor") -> Tuple[Optional[float], Optional[int]]:
        """
        Calculate Benchmark Index = (FTP_current / FTP_8_weeks_ago) - 1
        
        Returns (benchmark_index, ftp_8_weeks_ago)
        """
        if not current_ftp or not ftp_history:
            return None, None
        
        # Find FTP from ~8 weeks ago (56 days, with Â±7 day tolerance)
        target_date = datetime.now() - timedelta(days=56)
        earliest_acceptable = target_date - timedelta(days=7)
        latest_acceptable = target_date + timedelta(days=7)
        
        # Find the closest FTP entry to 8 weeks ago
        best_match_date = None
        best_match_diff = float('inf')
        
        for date_str, ftp in ftp_history.items():
            try:
                entry_date = datetime.strptime(date_str, "%Y-%m-%d")
                
                if earliest_acceptable <= entry_date <= latest_acceptable:
                    diff = abs((entry_date - target_date).days)
                    if diff < best_match_diff:
                        best_match_diff = diff
                        best_match_date = date_str
            except:
                continue
        
        if best_match_date:
            ftp_8_weeks_ago = ftp_history[best_match_date]
            benchmark_index = round((current_ftp / ftp_8_weeks_ago) - 1, 3)
            
            if self.debug:
                print(f"  Benchmark Index ({ftp_type}): {benchmark_index:+.1%} (FTP {ftp_8_weeks_ago} â†’ {current_ftp})")
            
            return benchmark_index, ftp_8_weeks_ago
        
        # No data from 8 weeks ago
        if self.debug:
            sorted_dates = sorted(ftp_history.keys())
            if sorted_dates:
                oldest_date = datetime.strptime(sorted_dates[0], "%Y-%m-%d")
                days_of_history = (datetime.now() - oldest_date).days
                print(f"  Benchmark Index ({ftp_type}) unavailable: only {days_of_history} days of history (need ~56)")
        
        return None, None
    
    def collect_training_data(self, days_back: int = 7, anonymize: bool = False) -> Dict:
        """Collect all training data for LLM analysis"""
        # Extended range for ACWR calculation (need 28 days minimum)
        days_for_acwr = 28
        oldest_extended = (datetime.now() - timedelta(days=days_for_acwr - 1)).strftime("%Y-%m-%d")
        oldest_display = (datetime.now() - timedelta(days=days_back - 1)).strftime("%Y-%m-%d")
        newest = datetime.now().strftime("%Y-%m-%d")
        today = datetime.now().strftime("%Y-%m-%d")
        
        print("Fetching athlete data...")
        athlete = self._intervals_get("")
        
        cycling_settings = None
        if athlete.get("sportSettings"):
            for sport in athlete["sportSettings"]:
                if "Ride" in sport.get("types", []) or "VirtualRide" in sport.get("types", []):
                    cycling_settings = sport
                    break
        
        # Fetch extended activity range for ACWR
        print(f"Fetching activities (extended {days_for_acwr} days for ACWR)...")
        activities_extended = self._intervals_get("activities", {"oldest": oldest_extended, "newest": newest})
        
        # Filter to display range for recent_activities
        activities_display = [a for a in activities_extended 
                             if a.get("start_date_local", "")[:10] >= oldest_display]
        
        print("Fetching wellness data...")
        wellness = self._intervals_get("wellness", {"oldest": oldest_display, "newest": newest})
        
        # Extended wellness for baselines (use full 28 days if available)
        wellness_extended = self._intervals_get("wellness", {"oldest": oldest_extended, "newest": newest})
        
        # Fetch today's wellness for live estimates (eFTP, W', P-max, VO2max, etc.)
        print("Fetching today's wellness (eFTP, W', P-max, VO2max)...")
        today_wellness = self._fetch_today_wellness()
        
        # Extract power model from wellness (accurate live estimates)
        power_model = self._extract_power_model_from_wellness(today_wellness)
        
        # Extract additional metrics from today's wellness
        vo2max = today_wellness.get("vo2max")
        
        # Get API values for fitness metrics (these include planned workouts!)
        api_ctl = today_wellness.get("ctl")
        api_atl = today_wellness.get("atl")
        api_ramp_rate = today_wellness.get("rampRate")
        
        # Fetch yesterday's wellness for decay fallback
        print("Fetching fitness metrics...")
        try:
            yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
            yesterday_wellness = self._intervals_get("wellness", {"oldest": yesterday, "newest": yesterday})
            yesterday_data = yesterday_wellness[0] if yesterday_wellness else {}
            
            # PMC decay constants
            ctl_decay = math.exp(-1/42)  # ~0.9765
            atl_decay = math.exp(-1/7)   # ~0.8668
            
            yesterday_ctl = yesterday_data.get("ctl")
            yesterday_atl = yesterday_data.get("atl")
            yesterday_ramp = yesterday_data.get("rampRate")
            
            # Decayed values = what fitness looks like with zero training today
            decayed_ctl = round(yesterday_ctl * ctl_decay, 2) if yesterday_ctl else None
            decayed_atl = round(yesterday_atl * atl_decay, 2) if yesterday_atl else None
            decayed_ramp = round(yesterday_ramp * ctl_decay, 2) if yesterday_ramp else None
        except:
            decayed_ctl = None
            decayed_atl = None
            decayed_ramp = None
            yesterday_ramp = None
        
        latest_wellness = wellness[-1] if wellness else {}
        
        # Fetch planned workouts (EXTENDED: include past 7 days for Consistency Index, 90 days ahead for race calendar)
        print("Fetching planned workouts (past + future for Consistency Index + race calendar)...")
        oldest_events = (datetime.now() - timedelta(days=days_back - 1)).strftime("%Y-%m-%d")
        newest_ahead = (datetime.now() + timedelta(days=90)).strftime("%Y-%m-%d")
        events = self._intervals_get("events", {"oldest": oldest_events, "newest": newest_ahead})
        
        # Split events into past (for consistency), near future (for planned workouts display), and all future (for race calendar)
        past_events = [e for e in events if e.get("start_date_local", "")[:10] <= today]
        future_events = [e for e in events if e.get("start_date_local", "")[:10] >= today]
        near_future_events = [e for e in future_events if e.get("start_date_local", "")[:10] <= (datetime.now() + timedelta(days=42)).strftime("%Y-%m-%d")]
        
        # Smart fitness metrics: same logic for CTL, ATL, TSB, and ramp rate
        # API values include planned workouts â†’ inflated if not yet completed
        # Decayed values = yesterday Ã— decay â†’ accurate baseline before any training today
        todays_planned = [e for e in events if e.get("start_date_local", "")[:10] == today]
        todays_activities = [a for a in activities_display if a.get("start_date_local", "")[:10] == today]
        
        if todays_planned and not todays_activities:
            # Planned workouts exist but nothing completed â†’ decay (API values are inflated)
            ctl = decayed_ctl
            atl = decayed_atl
            smart_ramp_rate = decayed_ramp if decayed_ramp else api_ramp_rate
            fitness_source = "Decayed from yesterday (today's planned workouts not yet completed)"
        else:
            # No planned workouts OR workouts completed â†’ API values are accurate
            ctl = round(api_ctl, 2) if api_ctl else decayed_ctl
            atl = round(api_atl, 2) if api_atl else decayed_atl
            smart_ramp_rate = round(api_ramp_rate, 2) if api_ramp_rate else decayed_ramp
            fitness_source = "From Intervals.icu API (reflects completed workouts)"
        
        tsb = round(ctl - atl, 2) if (ctl is not None and atl is not None) else None
        
        # Get both FTP values (user-set, not estimated)
        current_ftp_indoor = cycling_settings.get("indoor_ftp") if cycling_settings else None
        current_ftp_outdoor = cycling_settings.get("ftp") if cycling_settings else None
        
        # Load and update FTP history (tracks both indoor and outdoor)
        print("Updating FTP history...")
        ftp_history = self._load_ftp_history()
        ftp_history = self._save_ftp_history(ftp_history, current_ftp_indoor, current_ftp_outdoor)
        
        # Calculate Benchmark Index for both
        benchmark_index_indoor, ftp_8_weeks_ago_indoor = self._calculate_benchmark_index(
            current_ftp_indoor, ftp_history.get("indoor", {}), "indoor"
        )
        benchmark_index_outdoor, ftp_8_weeks_ago_outdoor = self._calculate_benchmark_index(
            current_ftp_outdoor, ftp_history.get("outdoor", {}), "outdoor"
        )
        
        # Calculate derived metrics for Section 11 compliance
        print("Calculating derived metrics...")
        derived_metrics = self._calculate_derived_metrics(
            activities_7d=activities_display,
            activities_28d=activities_extended,
            wellness_7d=wellness,
            wellness_extended=wellness_extended,
            current_ctl=ctl,
            current_atl=atl,
            current_tsb=tsb,
            past_events=past_events,
            activities_for_consistency=activities_display,
            power_model=power_model,
            benchmark_indoor=(benchmark_index_indoor, ftp_8_weeks_ago_indoor, current_ftp_indoor),
            benchmark_outdoor=(benchmark_index_outdoor, ftp_8_weeks_ago_outdoor, current_ftp_outdoor),
            vo2max=vo2max
        )
        
        # Generate alerts array (v3.3.0)
        print("Evaluating alert thresholds...")
        alerts = self._generate_alerts(
            derived_metrics=derived_metrics,
            wellness_7d=wellness,
            tss_7d_total=derived_metrics.get("tss_7d_total", 0),
            tss_28d_total=derived_metrics.get("tss_28d_total", 0)
        )
        
        if alerts:
            alarm_count = sum(1 for a in alerts if a["severity"] == "alarm")
            warning_count = sum(1 for a in alerts if a["severity"] == "warning")
            print(f"  âš ï¸  {len(alerts)} alerts: {alarm_count} alarm, {warning_count} warning")
        else:
            print("  âœ… No alerts â€” green light")
        
        # Build race calendar (v3.5.0)
        print("Building race calendar...")
        race_calendar = self._build_race_calendar(
            future_events=future_events,
            current_ctl=ctl,
            current_atl=atl,
            current_tsb=tsb,
            activities_7d=activities_display,
            today=today
        )
        
        # Add race-specific alerts
        race_alerts = self._generate_race_alerts(race_calendar)
        if race_alerts:
            alerts.extend(race_alerts)
            print(f"  ðŸ {len(race_alerts)} race alert(s) added")
        
        if race_calendar.get("race_week", {}).get("active"):
            rw = race_calendar["race_week"]
            print(f"  ðŸ Race week ACTIVE: {rw['current_day']} of '{rw['event_name']}'")
        elif race_calendar.get("taper_alert", {}).get("active"):
            nr = race_calendar.get("next_race", {})
            print(f"  ðŸ Taper alert: '{nr.get('name', '?')}' in {nr.get('days_until', '?')} days")
        elif race_calendar.get("next_race"):
            nr = race_calendar["next_race"]
            print(f"  ðŸ Next race: '{nr.get('name', '?')}' in {nr.get('days_until', '?')} days")
        else:
            print("  ðŸ No races in 90-day window")
        
        # History confidence (v3.3.0)
        history_info = self._get_history_confidence()
        
        data = {
            "READ_THIS_FIRST": {
                "instruction_for_ai": "DO NOT calculate totals from individual activities. Use the pre-calculated values in 'summary', 'weekly_summary', and 'derived_metrics' sections below. These are already computed accurately from the API data.",
                "data_period": f"Last {days_back} days (including today)",
                "extended_data_note": f"ACWR and baselines calculated from {days_for_acwr} days of data",
                "capability_metrics_note": "The 'capability' block in derived_metrics contains durability trend (aggregate decoupling 7d/28d) and TID comparison (7d vs 28d distribution drift). These measure HOW the athlete expresses fitness, not just load. Use these for coaching context alongside traditional load metrics. Durability trend direction matters more than absolute values.",
                "quick_stats": {
                    "total_training_hours": round(sum(act.get("moving_time", 0) for act in activities_display) / 3600, 2),
                    "total_activities": len(activities_display),
                    "total_tss": round(sum(act.get("icu_training_load", 0) for act in activities_display if act.get("icu_training_load")), 0)
                }
            },
            "metadata": {
                "athlete_id": "REDACTED" if anonymize else self.athlete_id,
                "last_updated": datetime.now().isoformat(),
                "data_range_days": days_back,
                "extended_range_days": days_for_acwr,
                "version": self.VERSION
            },
            "alerts": alerts,
            "history": history_info,
            "summary": self._compute_activity_summary(activities_display, days_back),
            "current_status": {
                "fitness": {
                    "ctl": ctl,
                    "atl": atl,
                    "tsb": tsb,
                    "ramp_rate": smart_ramp_rate,
                    "fitness_source": fitness_source
                },
                "thresholds": {
                    "ftp_outdoor": current_ftp_outdoor,
                    "ftp_indoor": current_ftp_indoor,
                    "eftp": power_model.get("eftp"),
                    "lthr": cycling_settings.get("lthr") if cycling_settings else None,
                    "max_hr": cycling_settings.get("max_hr") if cycling_settings else None,
                    "w_prime": power_model.get("w_prime"),
                    "w_prime_kj": power_model.get("w_prime_kj"),
                    "p_max": power_model.get("p_max"),
                    "vo2max": vo2max
                },
                "current_metrics": {
                    "weight_kg": latest_wellness.get("weight") or athlete.get("icu_weight"),
                    "resting_hr": latest_wellness.get("restingHR") or athlete.get("icu_resting_hr"),
                    "hrv": latest_wellness.get("hrv"),
                    "sleep_quality": latest_wellness.get("sleepQuality"),
                    "sleep_hours": round(latest_wellness.get("sleepSecs", 0) / 3600, 2) if latest_wellness.get("sleepSecs") else None
                }
            },
            "derived_metrics": derived_metrics,
            "recent_activities": self._format_activities(activities_display, anonymize),
            "wellness_data": self._format_wellness(wellness),
            "planned_workouts": self._format_events(near_future_events, anonymize),
            "weekly_summary": self._compute_weekly_summary(activities_display, wellness),
            "race_calendar": race_calendar
        }
        
        return data
    
    def _calculate_derived_metrics(self, activities_7d: List[Dict], activities_28d: List[Dict],
                                    wellness_7d: List[Dict], wellness_extended: List[Dict],
                                    current_ctl: float, current_atl: float, current_tsb: float,
                                    past_events: List[Dict], activities_for_consistency: List[Dict],
                                    power_model: Dict,
                                    benchmark_indoor: Tuple[Optional[float], Optional[int], Optional[int]],
                                    benchmark_outdoor: Tuple[Optional[float], Optional[int], Optional[int]],
                                    vo2max: float) -> Dict:
        """
        Calculate Section 11 derived metrics.
        
        Tier 1 (Primary): RI, baselines
        Tier 2 (Secondary): ACWR, Monotony, Strain, Stress Tolerance, Load-Recovery Ratio
        Tier 3 (Tertiary): Zone distribution, Polarisation, Phase Detection, Consistency, Benchmark
        
        Args:
            benchmark_indoor: (benchmark_index, ftp_8_weeks_ago, current_ftp) for indoor
            benchmark_outdoor: (benchmark_index, ftp_8_weeks_ago, current_ftp) for outdoor
        """
        
        # Unpack benchmark tuples
        benchmark_index_indoor, ftp_8_weeks_ago_indoor, current_ftp_indoor = benchmark_indoor
        benchmark_index_outdoor, ftp_8_weeks_ago_outdoor, current_ftp_outdoor = benchmark_outdoor
        
        # === DAILY TSS AGGREGATION ===
        daily_tss_7d = self._get_daily_tss(activities_7d, days=7)
        daily_tss_28d = self._get_daily_tss(activities_28d, days=28)
        
        tss_7d_total = sum(daily_tss_7d)
        tss_28d_total = sum(daily_tss_28d)
        
        # === ACWR (Acute:Chronic Workload Ratio) ===
        # Formula: (7-day avg TSS) / (28-day avg TSS)
        # Reference: Gabbett (2016) - "sweet spot" is 0.8-1.3
        acute_load = tss_7d_total / 7 if tss_7d_total else 0
        chronic_load = tss_28d_total / 28 if tss_28d_total else 0
        acwr = round(acute_load / chronic_load, 2) if chronic_load > 0 else None
        
        # === MONOTONY (Total) ===
        # Formula: mean(daily_tss) / stdev(daily_tss)
        # Reference: Foster (1998) - values >2.0 indicate increased illness risk
        if len(daily_tss_7d) > 1 and any(daily_tss_7d):
            mean_tss = statistics.mean(daily_tss_7d)
            try:
                stdev_tss = statistics.stdev(daily_tss_7d)
                monotony = round(mean_tss / stdev_tss, 2) if stdev_tss > 0 else None
            except:
                monotony = None
        else:
            monotony = None
            mean_tss = 0

        # === PRIMARY SPORT MONOTONY (v3.3.3) ===
        # Multi-sport athletes get inflated total monotony when cross-training
        # adds a consistent TSS floor across days. Per-sport monotony isolates
        # the actual load variation within each modality.
        daily_tss_by_sport = self._get_daily_tss_by_sport(activities_7d, days=7)
        primary_sport = None
        primary_sport_monotony = None
        primary_sport_tss_7d = None

        if daily_tss_by_sport:
            # Primary sport = highest 7-day TSS total
            sport_totals = {sport: sum(days) for sport, days in daily_tss_by_sport.items()}
            primary_sport = max(sport_totals, key=sport_totals.get) if sport_totals else None

            if primary_sport:
                primary_days = daily_tss_by_sport[primary_sport]
                primary_sport_tss_7d = round(sum(primary_days), 0)
                # Require â‰¥3 active days for meaningful monotony
                active_days = sum(1 for d in primary_days if d > 0)
                if active_days >= 3 and len(primary_days) > 1:
                    try:
                        ps_mean = statistics.mean(primary_days)
                        ps_stdev = statistics.stdev(primary_days)
                        primary_sport_monotony = round(ps_mean / ps_stdev, 2) if ps_stdev > 0 else None
                    except:
                        primary_sport_monotony = None

                if self.debug:
                    print(f"  Primary sport: {primary_sport} (TSS: {primary_sport_tss_7d})")
                    print(f"  Primary sport monotony: {primary_sport_monotony}")
                    print(f"  Total monotony: {monotony}")
                    if primary_sport_monotony and monotony and primary_sport_monotony < monotony:
                        print(f"  â†’ Multi-sport inflation detected ({monotony} total vs {primary_sport_monotony} primary)")

        # Determine effective monotony for alerts:
        # Use primary sport monotony when available and multi-sport detected,
        # fall back to total monotony otherwise
        is_multi_sport = len(daily_tss_by_sport) > 1
        effective_monotony = primary_sport_monotony if (is_multi_sport and primary_sport_monotony is not None) else monotony

        # === STRAIN ===
        # Formula: 7-day total TSS Ã— Monotony
        # Reference: Foster (1998) - values >3500-4000 associated with overtraining
        strain = round(tss_7d_total * monotony, 0) if monotony else None
        
        # === BASELINES (7-day and extended) ===
        hrv_values_7d = [w.get("hrv") for w in wellness_7d if self._is_valid_hrv(w.get("hrv"))]
        rhr_values_7d = [w.get("restingHR") for w in wellness_7d if w.get("restingHR")]
        
        hrv_baseline_7d = round(statistics.mean(hrv_values_7d), 1) if hrv_values_7d else None
        rhr_baseline_7d = round(statistics.mean(rhr_values_7d), 1) if rhr_values_7d else None
        
        # Extended baselines (for more stable reference)
        hrv_values_ext = [w.get("hrv") for w in wellness_extended if self._is_valid_hrv(w.get("hrv"))]
        rhr_values_ext = [w.get("restingHR") for w in wellness_extended if w.get("restingHR")]
        
        hrv_baseline_28d = round(statistics.mean(hrv_values_ext), 1) if hrv_values_ext else None
        rhr_baseline_28d = round(statistics.mean(rhr_values_ext), 1) if rhr_values_ext else None
        
        # === RECOVERY INDEX (RI) ===
        # Formula: (HRV_today / HRV_baseline) Ã· (RHR_today / RHR_baseline)
        # Interpretation: >1.0 = good recovery, <1.0 = poor recovery
        latest_hrv_raw = wellness_7d[-1].get("hrv") if wellness_7d else None
        latest_hrv = latest_hrv_raw if self._is_valid_hrv(latest_hrv_raw) else None
        latest_rhr = wellness_7d[-1].get("restingHR") if wellness_7d else None
        
        if latest_hrv and latest_rhr and hrv_baseline_7d and rhr_baseline_7d:
            hrv_ratio = latest_hrv / hrv_baseline_7d
            rhr_ratio = latest_rhr / rhr_baseline_7d
            ri = round(hrv_ratio / rhr_ratio, 2) if rhr_ratio > 0 else None
        else:
            ri = None
        
        # === STRESS TOLERANCE ===
        # Formula: (Strain Ã· Monotony) Ã· 100
        stress_tolerance = round((strain / monotony) / 100, 1) if strain and monotony else None
        
        # === LOAD-RECOVERY RATIO ===
        # Formula: 7-day Load Ã· (RI Ã— 100)
        load_recovery_ratio = round(tss_7d_total / (ri * 100), 1) if ri and ri > 0 else None
        
        # === ZONE AGGREGATION ===
        zone_totals = self._aggregate_zones(activities_7d)
        
        total_zone_time = zone_totals["total_time"]
        z1_time = zone_totals["z1_time"]
        z2_time = zone_totals["z2_time"]
        z3_time = zone_totals["z3_time"]
        z4_plus_time = zone_totals["z4_plus_time"]
        
        # === GREY ZONE PERCENTAGE (Z3 - to be minimized in polarized training) ===
        # Reference: Seiler - "too much pain for too little gain"
        grey_zone_percentage = round((z3_time / total_zone_time) * 100, 1) if total_zone_time > 0 else None
        
        # === QUALITY INTENSITY PERCENTAGE (Z4+ per Seiler's model) ===
        # Reference: Seiler's Zone 3 = above LT2 = Z4+ in 7-zone model
        # This is the "hard" work that should be ~20% in polarized training
        quality_intensity_percentage = round((z4_plus_time / total_zone_time) * 100, 1) if total_zone_time > 0 else None
        
        # === POLARISATION INDEX ===
        # Formula: (Z1 + Z2) / Total - measures how much time is "easy"
        # Target: ~80% for polarized training
        polarisation_index = round((z1_time + z2_time) / total_zone_time, 2) if total_zone_time > 0 else None
        
        # === SEILER TID (Training Intensity Distribution) ===
        # Dual calculation: all-sport and primary-sport (like monotony)
        # Uses correct 7â†’3 zone mapping per Treff et al. 2019
        seiler_tid_all = self._build_seiler_tid(activities_7d)

        seiler_tid_primary = None
        if primary_sport:
            seiler_tid_primary = self._build_seiler_tid(
                activities_7d, sport_family_filter=primary_sport
            )
            seiler_tid_primary["sport"] = primary_sport

        if self.debug:
            pi_all = seiler_tid_all.get("polarization_index")
            cls_all = seiler_tid_all.get("classification")
            print(f"  Seiler TID (all): {cls_all}, PI={pi_all}")
            if seiler_tid_primary:
                pi_ps = seiler_tid_primary.get("polarization_index")
                cls_ps = seiler_tid_primary.get("classification")
                print(f"  Seiler TID ({primary_sport}): {cls_ps}, PI={pi_ps}")

        # === SEILER TID 28d (Chronic Training Intensity Distribution) ===
        # Same method, wider window â€” for acute vs chronic TID comparison
        seiler_tid_28d_all = self._build_seiler_tid(activities_28d)

        seiler_tid_28d_primary = None
        if primary_sport:
            seiler_tid_28d_primary = self._build_seiler_tid(
                activities_28d, sport_family_filter=primary_sport
            )
            seiler_tid_28d_primary["sport"] = primary_sport

        if self.debug:
            pi_28d = seiler_tid_28d_all.get("polarization_index")
            cls_28d = seiler_tid_28d_all.get("classification")
            print(f"  Seiler TID 28d (all): {cls_28d}, PI={pi_28d}")

        # === TID COMPARISON (7d vs 28d drift detection) ===
        tid_comparison = self._calculate_tid_comparison(seiler_tid_all, seiler_tid_28d_all)

        # === DURABILITY TREND (aggregate decoupling) ===
        durability = self._calculate_durability(activities_7d, activities_28d)

        # === CONSISTENCY INDEX ===
        consistency_index, consistency_details = self._calculate_consistency_index(
            activities_for_consistency, past_events
        )
        
        # === HARD DAYS THIS WEEK ===
        # Zone ladder with cumulative thresholds (z+ = zone + all above)
        # z3+ >= 1800s, z4+ >= 600s, z5+ >= 300s, z6+ >= 120s, z7 >= 60s
        # Per Seiler's polarized model + Foster's session RPE scaling
        hard_days_this_week = 0
        activities_by_date_7d = {}
        for a in activities_7d:
            a_date = a.get("start_date_local", "")[:10]
            if a_date not in activities_by_date_7d:
                activities_by_date_7d[a_date] = []
            activities_by_date_7d[a_date].append(a)
        
        for date_str, day_acts in activities_by_date_7d.items():
            day_z3 = 0
            day_z4 = 0
            day_z5 = 0
            day_z6 = 0
            day_z7 = 0
            for a in day_acts:
                icu_zone_times = a.get("icu_zone_times", [])
                if icu_zone_times:
                    for zone in icu_zone_times:
                        zid = zone.get("id", "").lower()
                        secs = zone.get("secs", 0)
                        if zid == "z3":
                            day_z3 += secs
                        elif zid == "z4":
                            day_z4 += secs
                        elif zid == "z5":
                            day_z5 += secs
                        elif zid == "z6":
                            day_z6 += secs
                        elif zid == "z7":
                            day_z7 += secs
            # Zone ladder: cumulative thresholds (z+ = zone + all above)
            # Per Seiler's polarized model + Foster's session RPE scaling
            is_hard = (
                (day_z3 + day_z4 + day_z5 + day_z6 + day_z7) >= 1800 or  # z3+: 30 min tempo+
                (day_z4 + day_z5 + day_z6 + day_z7) >= 600 or            # z4+: 10 min threshold+
                (day_z5 + day_z6 + day_z7) >= 300 or                      # z5+: 5 min VO2max+
                (day_z6 + day_z7) >= 120 or                                # z6+: 2 min anaerobic+
                day_z7 >= 60                                                # z7:  1 min neuromuscular
            )
            if is_hard:
                hard_days_this_week += 1
        
        # === PHASE DETECTION ===
        phase_detected, phase_triggers = self._detect_phase(
            acwr=acwr,
            ri=ri,
            quality_intensity_pct=quality_intensity_percentage,
            hard_days_per_week=hard_days_this_week,
            strain=strain,
            monotony=monotony,
            tsb=current_tsb,
            ctl=current_ctl
        )
        
        # === SEASONAL CONTEXT ===
        seasonal_context = self._determine_seasonal_context()
        
        # === BENCHMARK SEASONAL EXPECTATION ===
        benchmark_expected_indoor = self._is_benchmark_expected(benchmark_index_indoor, seasonal_context)
        benchmark_expected_outdoor = self._is_benchmark_expected(benchmark_index_outdoor, seasonal_context)
        
        return {
            # Tier 1: Primary Readiness
            "recovery_index": ri,
            "hrv_baseline_7d": hrv_baseline_7d,
            "rhr_baseline_7d": rhr_baseline_7d,
            "hrv_baseline_28d": hrv_baseline_28d,
            "rhr_baseline_28d": rhr_baseline_28d,
            "latest_hrv": latest_hrv,
            "latest_rhr": latest_rhr,
            
            # Tier 2: Secondary Load Metrics
            "acwr": acwr,
            "acwr_interpretation": self._interpret_acwr(acwr),
            "monotony": monotony,
            "monotony_interpretation": self._interpret_monotony(monotony, effective_monotony, is_multi_sport),
            "primary_sport": primary_sport,
            "primary_sport_monotony": primary_sport_monotony,
            "primary_sport_tss_7d": primary_sport_tss_7d,
            "effective_monotony": effective_monotony,
            "multi_sport_detected": is_multi_sport,
            "strain": strain,
            "stress_tolerance": stress_tolerance,
            "load_recovery_ratio": load_recovery_ratio,
            "tss_7d_total": round(tss_7d_total, 0),
            "tss_28d_total": round(tss_28d_total, 0),
            
            # Tier 3: Zone Distribution (Seiler's Polarized Model)
            "zone_distribution_7d": {
                "z1_hours": round(z1_time / 3600, 2),
                "z2_hours": round(z2_time / 3600, 2),
                "z3_hours": round(z3_time / 3600, 2),
                "z4_plus_hours": round(z4_plus_time / 3600, 2),
                "total_hours": round(total_zone_time / 3600, 2)
            },
            "grey_zone_percentage": grey_zone_percentage,
            "grey_zone_note": "Gray Zone % (Z3/tempo) - minimize in polarized training",
            "quality_intensity_percentage": quality_intensity_percentage,
            "quality_intensity_note": "Quality Intensity % (Z4+/threshold+) - target ~20% in polarized training",
            "polarisation_index": polarisation_index,
            "polarisation_note": "Easy time (Z1+Z2) / Total - target ~80% in polarized training",
            "hard_days_this_week": hard_days_this_week,
            "hard_days_note": "Zone ladder: z3+ >= 30min, z4+ >= 10min, z5+ >= 5min, z6+ >= 2min, z7 >= 1min. Cumulative thresholds per Seiler/Foster â€” higher zones need less time to qualify as hard",
            
            # Tier 3: Seiler TID (Training Intensity Distribution)
            "seiler_tid_7d": seiler_tid_all,
            "seiler_tid_7d_primary": seiler_tid_primary,
            "seiler_tid_28d": seiler_tid_28d_all,
            "seiler_tid_28d_primary": seiler_tid_28d_primary,
            
            # Capability metrics (how fitness is expressed, not just load)
            "capability": {
                "durability": durability,
                "tid_comparison": tid_comparison,
            },
            
            # Tier 3: Consistency & Compliance
            "consistency_index": consistency_index,
            "consistency_details": consistency_details,
            
            # Phase & Context
            "phase_detected": phase_detected,
            "phase_triggers": phase_triggers,
            "seasonal_context": seasonal_context,
            
            # Benchmark & FTP Progression (Indoor)
            "benchmark_indoor": {
                "current_ftp": current_ftp_indoor,
                "ftp_8_weeks_ago": ftp_8_weeks_ago_indoor,
                "benchmark_index": benchmark_index_indoor,
                "benchmark_percentage": f"{benchmark_index_indoor:+.1%}" if benchmark_index_indoor is not None else None,
                "seasonal_expected": benchmark_expected_indoor
            },
            # Benchmark & FTP Progression (Outdoor)
            "benchmark_outdoor": {
                "current_ftp": current_ftp_outdoor,
                "ftp_8_weeks_ago": ftp_8_weeks_ago_outdoor,
                "benchmark_index": benchmark_index_outdoor,
                "benchmark_percentage": f"{benchmark_index_outdoor:+.1%}" if benchmark_index_outdoor is not None else None,
                "seasonal_expected": benchmark_expected_outdoor
            },
            
            # Power Model (from API - accurate live estimates)
            "eftp": power_model.get("eftp"),
            "w_prime": power_model.get("w_prime"),
            "w_prime_kj": power_model.get("w_prime_kj"),
            "p_max": power_model.get("p_max"),
            "power_model_source": power_model.get("source"),
            
            # Additional wellness metrics (from API)
            "vo2max": vo2max,
            
            # Validation metadata
            "calculation_timestamp": datetime.now().isoformat(),
            "data_quality": {
                "hrv_data_points": len(hrv_values_7d),
                "rhr_data_points": len(rhr_values_7d),
                "activities_7d": len(activities_7d),
                "activities_28d": len(activities_28d),
                "planned_workouts_7d": len(past_events),
                "ftp_history_days": self._get_ftp_history_span()
            }
        }
    
    def _interpret_acwr(self, acwr: float) -> Optional[str]:
        """Interpret ACWR value per Gabbett guidelines"""
        if acwr is None:
            return None
        if acwr < 0.8:
            return "undertraining"
        elif acwr <= 1.3:
            return "optimal"
        elif acwr <= 1.5:
            return "caution"
        else:
            return "danger"

    def _interpret_monotony(self, total_monotony: float, effective_monotony: float, is_multi_sport: bool) -> Optional[str]:
        """
        Interpret monotony with multi-sport awareness.
        When multi-sport training inflates total monotony, the interpretation
        reflects the effective (primary sport) value instead.
        """
        if effective_monotony is None:
            return None
        if is_multi_sport and total_monotony and effective_monotony < total_monotony:
            # Multi-sport inflation detected
            if effective_monotony > 2.0:
                return f"elevated (primary sport {effective_monotony}, total {total_monotony} inflated by multi-sport)"
            else:
                return f"normal (primary sport {effective_monotony}, total {total_monotony} inflated by multi-sport)"
        else:
            if effective_monotony > 2.0:
                return "elevated"
            else:
                return "normal"

    def _calculate_consistency_index(self, activities: List[Dict], 
                                      past_events: List[Dict]) -> Tuple[Optional[float], Dict]:
        """
        Calculate Consistency Index = Completed Workout Days / Planned Workout Days
        
        Matches by date (not individual workouts) since multiple workouts can be planned per day.
        """
        # Get unique dates with planned workouts (only WORKOUT type)
        planned_dates = set()
        for event in past_events:
            if event.get("category") == "WORKOUT":
                date_str = event.get("start_date_local", "")[:10]
                if date_str:
                    planned_dates.add(date_str)
        
        # Get unique dates with completed activities (cycling only for fair comparison)
        completed_dates = set()
        cycling_types = {"Ride", "VirtualRide", "MountainBikeRide", "GravelRide"}
        
        for activity in activities:
            if activity.get("type") in cycling_types:
                date_str = activity.get("start_date_local", "")[:10]
                if date_str:
                    completed_dates.add(date_str)
        
        # Calculate overlap
        matched_dates = planned_dates & completed_dates
        
        if not planned_dates:
            return None, {
                "planned_days": 0,
                "completed_days": len(completed_dates),
                "matched_days": 0,
                "note": "No planned workouts in period"
            }
        
        consistency_index = round(len(matched_dates) / len(planned_dates), 2)
        
        return consistency_index, {
            "planned_days": len(planned_dates),
            "completed_days": len(completed_dates),
            "matched_days": len(matched_dates),
            "planned_dates": sorted(list(planned_dates)),
            "completed_dates": sorted(list(completed_dates))
        }
    
    def _is_benchmark_expected(self, benchmark_index: Optional[float], 
                                seasonal_context: str) -> Optional[bool]:
        """
        Determine if the benchmark index is within expected range for the season.
        """
        if benchmark_index is None:
            return None
        
        expectations = {
            "Off-season / Transition": (-0.05, -0.02),
            "Early Base": (-0.02, 0.01),
            "Late Base / Build": (0.02, 0.05),
            "Build / Early Race Season": (0.01, 0.04),
            "Peak Race Season": (0.01, 0.03),
            "Late Season / Transition": (-0.03, 0.00),
        }
        
        if seasonal_context in expectations:
            low, high = expectations[seasonal_context]
            return low <= benchmark_index <= high
        
        return None
    
    def _get_ftp_history_span(self) -> Dict[str, int]:
        """Get the number of days of FTP history available for indoor and outdoor"""
        ftp_history = self._load_ftp_history()
        
        result = {"indoor": 0, "outdoor": 0}
        
        for ftp_type in ["indoor", "outdoor"]:
            history = ftp_history.get(ftp_type, {})
            if not history:
                continue
            
            sorted_dates = sorted(history.keys())
            if len(sorted_dates) < 2:
                continue
            
            try:
                oldest = datetime.strptime(sorted_dates[0], "%Y-%m-%d")
                newest = datetime.strptime(sorted_dates[-1], "%Y-%m-%d")
                result[ftp_type] = (newest - oldest).days
            except:
                continue
        
        return result
    
    def _get_daily_tss(self, activities: List[Dict], days: int) -> List[float]:
        """Aggregate TSS by day for the specified number of days"""
        daily_tss = defaultdict(float)
        
        for act in activities:
            date_str = act.get("start_date_local", "")[:10]
            tss = act.get("icu_training_load") or 0
            daily_tss[date_str] += tss
        
        # Create array for last N days (including days with 0 TSS)
        result = []
        for i in range(days - 1, -1, -1):
            date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
            result.append(daily_tss.get(date, 0))
        
        return result

    def _get_daily_tss_by_sport(self, activities: List[Dict], days: int) -> Dict[str, List[float]]:
        """
        Aggregate TSS by day AND sport family for per-sport monotony calculation.
        Returns dict of sport_family â†’ [daily_tss_day1, daily_tss_day2, ...] (N elements).
        Only includes sport families that have at least one activity with TSS > 0.
        Sport families are mapped via SPORT_FAMILIES class constant.
        Unmapped activity types are grouped as "other".
        """
        # Collect all sport families present and their daily TSS
        sport_daily_tss = defaultdict(lambda: defaultdict(float))

        for act in activities:
            date_str = act.get("start_date_local", "")[:10]
            tss = act.get("icu_training_load") or 0
            if tss <= 0:
                continue
            activity_type = act.get("type", "Unknown")
            sport_family = self.SPORT_FAMILIES.get(activity_type, "other")
            sport_daily_tss[sport_family][date_str] += tss

        # Build daily arrays for each sport family (including 0 days)
        result = {}
        for sport_family, daily_dict in sport_daily_tss.items():
            daily_array = []
            for i in range(days - 1, -1, -1):
                date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
                daily_array.append(daily_dict.get(date, 0))
            result[sport_family] = daily_array

        return result

    def _aggregate_zones(self, activities: List[Dict]) -> Dict:
        """
        Aggregate zone times across all activities.
        
        Returns separate Z1, Z2, Z3, and Z4+ times for proper polarization analysis.
        Per Seiler's model:
        - Z1-Z2: Easy (below LT1)
        - Z3: Grey zone / Tempo (between LT1 and LT2) - to be minimized
        - Z4+: Hard / Quality (above LT2) - ~20% target
        """
        z1_time = 0
        z2_time = 0
        z3_time = 0
        z4_plus_time = 0
        total_time = 0
        
        for act in activities:
            zones = None
            
            # Check for zone data in raw activity
            icu_zone_times = act.get("icu_zone_times", [])
            icu_hr_zone_times = act.get("icu_hr_zone_times", [])
            
            # Power zones (preferred for cycling)
            if icu_zone_times:
                pz = {}
                for zone in icu_zone_times:
                    zone_id = zone.get("id", "").lower()
                    secs = zone.get("secs", 0)
                    if zone_id in ["z1", "z2", "z3", "z4", "z5", "z6", "z7"]:
                        pz[zone_id] = secs
                if pz:
                    zones = pz
            
            # HR zones (fallback)
            if not zones and icu_hr_zone_times:
                zone_labels = ["z1", "z2", "z3", "z4", "z5", "z6", "z7"]
                hz = {}
                for idx, secs in enumerate(icu_hr_zone_times):
                    if idx < len(zone_labels) and secs:
                        hz[zone_labels[idx]] = secs
                if hz:
                    zones = hz
            
            if zones:
                z1_time += zones.get("z1", 0)
                z2_time += zones.get("z2", 0)
                z3_time += zones.get("z3", 0)
                z4_plus_time += (zones.get("z4", 0) + zones.get("z5", 0) + 
                               zones.get("z6", 0) + zones.get("z7", 0))
                total_time += sum(zones.values())
        
        return {
            "z1_time": z1_time,
            "z2_time": z2_time,
            "z3_time": z3_time,
            "z4_plus_time": z4_plus_time,
            "total_time": total_time
        }
    
    # === SEILER TID (Training Intensity Distribution) v3.4.0 ===

    def _aggregate_seiler_zones(self, activities: List[Dict],
                                sport_family_filter: str = None) -> Dict:
        """
        Aggregate 7-zone times into Seiler 3-zone model.

        Mapping (per Treff et al. 2019):
            Seiler Z1 = z1 + z2  (below LT1)
            Seiler Z2 = z3       (between LT1 and LT2)
            Seiler Z3 = z4 + z5 + z6 + z7  (above LT2)

        Uses power zones when available, falls back to HR zones.

        Args:
            activities: List of activity dicts with zone data
            sport_family_filter: If set, only include activities matching
                                 this sport family (from SPORT_FAMILIES)

        Returns dict with z1_seconds, z2_seconds, z3_seconds, total_seconds
        """
        sz1 = 0
        sz2 = 0
        sz3 = 0

        for act in activities:
            # Apply sport family filter if specified
            if sport_family_filter:
                activity_type = act.get("type", "Unknown")
                if self.SPORT_FAMILIES.get(activity_type, "other") != sport_family_filter:
                    continue

            zones = None

            # Power zones (preferred)
            icu_zone_times = act.get("icu_zone_times", [])
            if icu_zone_times:
                pz = {}
                for zone in icu_zone_times:
                    zone_id = zone.get("id", "").lower()
                    secs = zone.get("secs", 0)
                    if zone_id in ["z1", "z2", "z3", "z4", "z5", "z6", "z7"]:
                        pz[zone_id] = secs
                if pz:
                    zones = pz

            # HR zones (fallback)
            if not zones:
                icu_hr_zone_times = act.get("icu_hr_zone_times", [])
                if icu_hr_zone_times:
                    zone_labels = ["z1", "z2", "z3", "z4", "z5", "z6", "z7"]
                    hz = {}
                    for idx, secs in enumerate(icu_hr_zone_times):
                        if idx < len(zone_labels) and secs:
                            hz[zone_labels[idx]] = secs
                    if hz:
                        zones = hz

            if zones:
                sz1 += zones.get("z1", 0) + zones.get("z2", 0)
                sz2 += zones.get("z3", 0)
                sz3 += (zones.get("z4", 0) + zones.get("z5", 0) +
                        zones.get("z6", 0) + zones.get("z7", 0))

        total = sz1 + sz2 + sz3
        return {
            "z1_seconds": sz1,
            "z2_seconds": sz2,
            "z3_seconds": sz3,
            "total_seconds": total
        }

    def _calculate_polarization_index(self, z1_frac: float, z2_frac: float,
                                       z3_frac: float) -> Optional[float]:
        """
        Calculate Treff Polarization Index.

        Formula: PI = log10((Z1 / Z2) Ã— Z3 Ã— 100)

        Rules (per Treff et al. 2019 + updated literature):
        - Only compute when Z1 > Z3 > Z2 and Z3 >= 0.01
        - If Z2 = 0 but structure is polarized: substitute Z2 = 0.01
        - Otherwise: return None (not a polarization score)
        """
        if z3_frac < 0.01:
            return None
        if not (z1_frac > z3_frac > z2_frac):
            return None

        # Handle Z2 = 0 with substitution (per updated PI formulation)
        effective_z2 = z2_frac if z2_frac > 0 else 0.01

        try:
            raw = (z1_frac / effective_z2) * z3_frac * 100
            if raw <= 0:
                return None
            return round(math.log10(raw), 2)
        except (ValueError, ZeroDivisionError):
            return None

    def _classify_tid(self, z1_frac: float, z2_frac: float,
                      z3_frac: float, pi: Optional[float]) -> str:
        """
        Classify Training Intensity Distribution.

        Explicit priority order (to avoid overlaps):
        1. Z3 < 0.01 and Z1 largest â†’ Base
        2. Z1 > Z3 > Z2 and PI > 2.0 â†’ Polarized
        3. Z1 > Z2 > Z3 â†’ Pyramidal
        4. Z2 largest â†’ Threshold
        5. Z3 largest â†’ High Intensity
        """
        # 1. Base: Z3 near zero, Z1 dominant
        if z3_frac < 0.01 and z1_frac >= z2_frac and z1_frac >= z3_frac:
            return "Base"

        # 2. Polarized: Z1 > Z3 > Z2 and PI > 2.0
        if z1_frac > z3_frac > z2_frac and pi is not None and pi > 2.0:
            return "Polarized"

        # 3. Pyramidal: Z1 > Z2 > Z3
        if z1_frac > z2_frac > z3_frac:
            return "Pyramidal"

        # 4. Threshold: Z2 dominant
        if z2_frac >= z1_frac and z2_frac >= z3_frac:
            return "Threshold"

        # 5. High Intensity: Z3 dominant
        if z3_frac >= z1_frac and z3_frac >= z2_frac:
            return "High Intensity"

        # Fallback: polarized structure but PI <= 2.0
        return "Pyramidal"

    def _build_seiler_tid(self, activities: List[Dict],
                          sport_family_filter: str = None) -> Dict:
        """
        Build complete Seiler TID structure for given activities.

        Returns dict with:
            z1_seconds, z2_seconds, z3_seconds
            z1_pct, z2_pct, z3_pct
            polarization_index (float or null)
            classification (string)
        """
        zones = self._aggregate_seiler_zones(activities, sport_family_filter)
        total = zones["total_seconds"]

        if total == 0:
            return {
                "z1_seconds": 0,
                "z2_seconds": 0,
                "z3_seconds": 0,
                "z1_pct": None,
                "z2_pct": None,
                "z3_pct": None,
                "polarization_index": None,
                "classification": None
            }

        z1_frac = zones["z1_seconds"] / total
        z2_frac = zones["z2_seconds"] / total
        z3_frac = zones["z3_seconds"] / total

        pi = self._calculate_polarization_index(z1_frac, z2_frac, z3_frac)
        classification = self._classify_tid(z1_frac, z2_frac, z3_frac, pi)

        return {
            "z1_seconds": zones["z1_seconds"],
            "z2_seconds": zones["z2_seconds"],
            "z3_seconds": zones["z3_seconds"],
            "z1_pct": round(z1_frac * 100, 1),
            "z2_pct": round(z2_frac * 100, 1),
            "z3_pct": round(z3_frac * 100, 1),
            "polarization_index": pi,
            "classification": classification
        }

    def _calculate_durability(self, activities_7d: List[Dict],
                               activities_28d: List[Dict]) -> Dict:
        """
        Calculate aggregate decoupling as a durability trend.

        Filters to steady-state power sessions only:
        - decoupling is not None
        - variability_index is not None and > 0 and <= 1.05
        - moving_time >= 5400 (90 minutes)

        Per Maunder et al. (2021), Rothschild et al. (2025): meaningful
        cardiac drift requires prolonged exercise. 90 min is the practical
        field floor where drift becomes detectable.

        Negative decoupling is included â€” it indicates HR drifted down
        relative to power (strong durability or cooling conditions).

        Returns dict with 7d/28d means, high-drift counts, qualifying
        session counts, and trend direction.
        """
        def _filter_qualifying(activities: List[Dict]) -> List[float]:
            """Return decoupling values from qualifying sessions."""
            qualifying = []
            for act in activities:
                # Raw API field names (before _format_activities)
                dec = act.get("icu_hr_decoupling") or act.get("decoupling")
                vi = act.get("icu_variability_index")
                mt = act.get("moving_time", 0) or 0

                if (dec is not None
                        and vi is not None
                        and vi > 0
                        and vi <= 1.05
                        and mt >= 5400):
                    qualifying.append(dec)
            return qualifying

        vals_7d = _filter_qualifying(activities_7d)
        vals_28d = _filter_qualifying(activities_28d)

        # Compute means (need >= 2 qualifying sessions)
        mean_7d = round(statistics.mean(vals_7d), 2) if len(vals_7d) >= 2 else None
        mean_28d = round(statistics.mean(vals_28d), 2) if len(vals_28d) >= 2 else None

        # High drift counts (> 5%)
        high_drift_7d = sum(1 for v in vals_7d if v > 5.0)
        high_drift_28d = sum(1 for v in vals_28d if v > 5.0)

        # Trend (requires both windows)
        trend = None
        if mean_7d is not None and mean_28d is not None:
            delta = mean_7d - mean_28d
            if delta < -1.0:
                trend = "improving"
            elif delta > 1.0:
                trend = "declining"
            else:
                trend = "stable"

        if self.debug:
            print(f"  Durability: 7d={mean_7d} ({len(vals_7d)} sessions), "
                  f"28d={mean_28d} ({len(vals_28d)} sessions), trend={trend}")

        return {
            "mean_decoupling_7d": mean_7d,
            "mean_decoupling_28d": mean_28d,
            "high_drift_count_7d": high_drift_7d,
            "high_drift_count_28d": high_drift_28d,
            "qualifying_sessions_7d": len(vals_7d),
            "qualifying_sessions_28d": len(vals_28d),
            "trend": trend,
            "note": ("Steady-state power sessions only (VI <= 1.05, VI > 0, "
                     ">= 90min, power data). Negative decoupling = strong "
                     "durability. Trend compares 7d vs 28d mean "
                     "(+/-1% = stable).")
        }

    def _calculate_tid_comparison(self, seiler_tid_7d: Dict,
                                   seiler_tid_28d: Dict) -> Dict:
        """
        Compare 7d vs 28d Seiler TID to detect distribution drift.

        Drift categories:
        - consistent: 7d and 28d classification match
        - shifting: 7d and 28d classification differ
        - acute_depolarization: 7d PI < 2.0 AND 28d PI >= 2.0

        Returns dict with classifications, PI values, delta, and drift.
        """
        cls_7d = seiler_tid_7d.get("classification")
        cls_28d = seiler_tid_28d.get("classification")
        pi_7d = seiler_tid_7d.get("polarization_index")
        pi_28d = seiler_tid_28d.get("polarization_index")

        # Null handling: if either window has no data, no comparison
        if cls_7d is None or cls_28d is None:
            return {
                "classification_7d": cls_7d,
                "classification_28d": cls_28d,
                "pi_7d": pi_7d,
                "pi_28d": pi_28d,
                "pi_delta": None,
                "drift": None,
                "note": ("Compares 7d vs 28d Seiler TID to detect "
                         "distribution shifts. Insufficient data in "
                         "one or both windows.")
            }

        # PI delta (positive = more polarized acutely)
        pi_delta = None
        if pi_7d is not None and pi_28d is not None:
            pi_delta = round(pi_7d - pi_28d, 2)

        # Drift classification
        # Check acute_depolarization first (more specific than shifting)
        if (pi_7d is not None and pi_28d is not None
                and pi_7d < 2.0 and pi_28d >= 2.0):
            drift = "acute_depolarization"
        elif cls_7d != cls_28d:
            drift = "shifting"
        else:
            drift = "consistent"

        if self.debug:
            print(f"  TID comparison: 7d={cls_7d} (PI={pi_7d}), "
                  f"28d={cls_28d} (PI={pi_28d}), drift={drift}")

        return {
            "classification_7d": cls_7d,
            "classification_28d": cls_28d,
            "pi_7d": pi_7d,
            "pi_28d": pi_28d,
            "pi_delta": pi_delta,
            "drift": drift,
            "note": ("Compares 7d vs 28d Seiler TID to detect "
                     "distribution shifts. pi_delta positive = "
                     "more polarized acutely.")
        }

    def _detect_phase(self, acwr: float, ri: float, quality_intensity_pct: float,
                      hard_days_per_week: int,
                      strain: float, monotony: float, tsb: float, ctl: float) -> Tuple[str, List[str]]:
        """
        Detect current training phase based on Section 11 Phase Detection Criteria
        
        Uses both time-based quality intensity % AND session-based hard days/week.
        For high-volume athletes (10+ hrs/week), time-based metrics undercount intensity
        because hard sessions are diluted by volume. Session count provides the correction.
        """
        triggers = []
        
        # Check for Overreached first (safety)
        if acwr and acwr > 1.3:
            triggers.append(f"ACWR {acwr} > 1.3")
        if strain and strain > 3500:
            triggers.append(f"Strain {strain} > 3500")
        if ri and ri < 0.6:
            triggers.append(f"RI {ri} < 0.6")
        if monotony and monotony > 2.5:
            triggers.append(f"Monotony {monotony} > 2.5")
        
        if len(triggers) >= 2 or (ri and ri < 0.6):
            return "Overreached", triggers
        
        # Recovery phase
        if tsb and tsb > 10:
            triggers = [f"TSB {tsb} > +10"]
            return "Recovery", triggers
        
        # Taper phase
        if tsb and tsb > 0 and ctl:
            if 0 < tsb <= 10:
                triggers = [f"TSB {tsb} positive", "CTL stable/declining"]
                return "Taper", triggers
        
        # Build phase â€” by time OR by session count
        # High-volume athletes may show low quality % but still train 2+ hard days/week
        build_by_time = (quality_intensity_pct and 15 <= quality_intensity_pct <= 25)
        build_by_sessions = (hard_days_per_week >= 2)
        
        if acwr and 0.8 <= acwr <= 1.3:
            if build_by_time or build_by_sessions:
                triggers = [f"ACWR {acwr} in 0.8-1.3"]
                if build_by_time:
                    triggers.append(f"Quality Intensity {quality_intensity_pct}% in 15-25%")
                if build_by_sessions:
                    triggers.append(f"Hard days {hard_days_per_week}/week >= 2")
                return "Build", triggers
        
        # Base phase â€” low intensity by BOTH time and session count
        if acwr and 0.8 <= acwr < 1.0:
            triggers = [f"ACWR {acwr} in 0.8-1.0"]
            if quality_intensity_pct is not None:
                triggers.append(f"Quality Intensity {quality_intensity_pct}% < 15%")
            if hard_days_per_week is not None:
                triggers.append(f"Hard days {hard_days_per_week}/week <= 1")
            return "Base", triggers
        
        # Peak phase â€” high intensity with controlled load
        peak_by_time = (quality_intensity_pct and quality_intensity_pct > 20)
        peak_by_sessions = (hard_days_per_week >= 3)
        
        if acwr and acwr >= 1.0 and (peak_by_time or peak_by_sessions):
            triggers = [f"ACWR {acwr} >= 1.0"]
            if peak_by_time:
                triggers.append(f"Quality Intensity {quality_intensity_pct}% > 20%")
            if peak_by_sessions:
                triggers.append(f"Hard days {hard_days_per_week}/week >= 3")
            return "Peak", triggers
        
        return "Indeterminate", ["Insufficient data for phase detection"]
    
    def _determine_seasonal_context(self) -> str:
        """
        Determine seasonal context based on current month.
        Assumes Northern Hemisphere cycling calendar.
        """
        month = datetime.now().month
        
        if month in [11, 12]:
            return "Off-season / Transition"
        elif month in [1, 2]:
            return "Early Base"
        elif month in [3, 4]:
            return "Late Base / Build"
        elif month in [5, 6]:
            return "Build / Early Race Season"
        elif month in [7, 8]:
            return "Peak Race Season"
        elif month in [9, 10]:
            return "Late Season / Transition"
        else:
            return "Unknown"
    
    # === ALERTS SYSTEM (v3.3.0) ===
    
    def _generate_alerts(self, derived_metrics: Dict, wellness_7d: List[Dict],
                         tss_7d_total: float, tss_28d_total: float) -> List[Dict]:
        """
        Generate graduated alerts array based on Section 11 v11.4 thresholds.
        
        Severity levels: "info" â†’ "warning" â†’ "alarm"
        Empty array = green light.
        
        Monotony alerts use effective_monotony (primary sport when multi-sport
        detected) to avoid false positives from cross-training TSS floor inflation.
        """
        alerts = []
        
        acwr = derived_metrics.get("acwr")
        monotony = derived_metrics.get("monotony")
        effective_monotony = derived_metrics.get("effective_monotony")
        primary_sport = derived_metrics.get("primary_sport")
        primary_sport_monotony = derived_metrics.get("primary_sport_monotony")
        is_multi_sport = derived_metrics.get("multi_sport_detected", False)
        strain = derived_metrics.get("strain")
        ri = derived_metrics.get("recovery_index")
        latest_hrv = derived_metrics.get("latest_hrv")
        latest_rhr = derived_metrics.get("latest_rhr")
        hrv_baseline_7d = derived_metrics.get("hrv_baseline_7d")
        rhr_baseline_7d = derived_metrics.get("rhr_baseline_7d")
        
        # --- ACWR Alerts ---
        if acwr is not None:
            if acwr <= 0.75 or acwr >= 1.35:
                alerts.append({
                    "metric": "acwr",
                    "value": acwr,
                    "severity": "alarm",
                    "threshold": "0.75 / 1.35",
                    "context": f"ACWR {acwr} outside safe range. Injury/overreach risk elevated.",
                    "persistence_days": None,
                    "tier": 2
                })
            elif acwr <= 0.8 or acwr >= 1.3:
                alerts.append({
                    "metric": "acwr",
                    "value": acwr,
                    "severity": "warning",
                    "threshold": "0.8 / 1.3",
                    "context": f"ACWR {acwr} at edge of optimal range. Monitor closely. Alarm at 0.75/1.35.",
                    "persistence_days": None,
                    "tier": 2
                })
        
        # --- Monotony Alerts (with deload context + multi-sport awareness) ---
        # Use effective_monotony for alert thresholds. When multi-sport training
        # is detected and primary sport monotony is lower than total, the effective
        # value reflects the actual training load variation of the main modality.
        if effective_monotony is not None:
            deload_context = self._detect_deload_context(tss_7d_total, tss_28d_total)

            # Build context string for multi-sport cases
            multi_sport_note = ""
            if is_multi_sport and primary_sport_monotony is not None and monotony is not None and primary_sport_monotony < monotony:
                multi_sport_note = f" (total monotony {monotony} inflated by multi-sport training; {primary_sport} monotony {primary_sport_monotony} used for alerting)"

            if effective_monotony >= 2.5:
                if deload_context:
                    alerts.append({
                        "metric": "monotony",
                        "value": effective_monotony,
                        "severity": "info",
                        "threshold": 2.5,
                        "context": f"Monotony {effective_monotony} â‰¥ 2.5 but {deload_context}. Structural artifact, not overuse risk. Will normalize as 7-day window rolls forward.{multi_sport_note}",
                        "persistence_days": None,
                        "tier": 2
                    })
                else:
                    alerts.append({
                        "metric": "monotony",
                        "value": effective_monotony,
                        "severity": "alarm",
                        "threshold": 2.5,
                        "context": f"Monotony {effective_monotony} â‰¥ 2.5. Overuse risk elevated. Vary training load.{multi_sport_note}",
                        "persistence_days": None,
                        "tier": 2
                    })
            elif effective_monotony >= 2.3:
                if deload_context:
                    alerts.append({
                        "metric": "monotony",
                        "value": effective_monotony,
                        "severity": "info",
                        "threshold": 2.3,
                        "context": f"Monotony {effective_monotony} approaching threshold but {deload_context}. Expected, not actionable.{multi_sport_note}",
                        "persistence_days": None,
                        "tier": 2
                    })
                else:
                    alerts.append({
                        "metric": "monotony",
                        "value": effective_monotony,
                        "severity": "warning",
                        "threshold": 2.3,
                        "context": f"Monotony {effective_monotony} approaching overuse threshold. Alarm at 2.5.{multi_sport_note}",
                        "persistence_days": None,
                        "tier": 2
                    })
        
        # --- Strain Alerts ---
        if strain is not None and strain > 3500:
            alerts.append({
                "metric": "strain",
                "value": strain,
                "severity": "alarm",
                "threshold": 3500,
                "context": f"Strain {strain} > 3500. High cumulative stress. Consider load reduction.",
                "persistence_days": None,
                "tier": 2
            })
        
        # --- Recovery Index Alerts ---
        if ri is not None:
            if ri < 0.6:
                alerts.append({
                    "metric": "recovery_index",
                    "value": ri,
                    "severity": "alarm",
                    "threshold": 0.6,
                    "context": f"RI {ri} < 0.6. Immediate deload required.",
                    "persistence_days": None,
                    "tier": 1
                })
            elif ri < 0.7:
                alerts.append({
                    "metric": "recovery_index",
                    "value": ri,
                    "severity": "warning",
                    "threshold": 0.7,
                    "context": f"RI {ri} < 0.7. Monitor â€” if persists >3 days, deload review required.",
                    "persistence_days": None,
                    "tier": 1
                })
        
        # --- HRV Alerts ---
        if latest_hrv and hrv_baseline_7d and hrv_baseline_7d > 0:
            hrv_change_pct = ((latest_hrv - hrv_baseline_7d) / hrv_baseline_7d) * 100
            if hrv_change_pct <= -20:
                # Check persistence: count consecutive days with HRV â†“>20%
                hrv_low_days = self._count_hrv_low_days(wellness_7d, hrv_baseline_7d)
                
                if hrv_low_days > 2:
                    alerts.append({
                        "metric": "hrv",
                        "value": round(latest_hrv, 1),
                        "severity": "alarm",
                        "threshold": f"â†“>20% vs baseline ({round(hrv_baseline_7d, 1)})",
                        "context": f"HRV {round(latest_hrv, 1)} is {round(abs(hrv_change_pct), 1)}% below baseline, persisting {hrv_low_days} days.",
                        "persistence_days": hrv_low_days,
                        "tier": 1
                    })
                else:
                    alerts.append({
                        "metric": "hrv",
                        "value": round(latest_hrv, 1),
                        "severity": "warning",
                        "threshold": f"â†“>20% vs baseline ({round(hrv_baseline_7d, 1)})",
                        "context": f"HRV {round(latest_hrv, 1)} is {round(abs(hrv_change_pct), 1)}% below baseline. Monitor â€” alarm if persists >2 days.",
                        "persistence_days": hrv_low_days,
                        "tier": 1
                    })
        
        # --- RHR Alerts ---
        if latest_rhr and rhr_baseline_7d and rhr_baseline_7d > 0:
            rhr_change = latest_rhr - rhr_baseline_7d
            if rhr_change >= 5:
                # Check persistence
                rhr_high_days = self._count_rhr_high_days(wellness_7d, rhr_baseline_7d)
                
                if rhr_high_days > 2:
                    alerts.append({
                        "metric": "rhr",
                        "value": round(latest_rhr, 1),
                        "severity": "alarm",
                        "threshold": f"â†‘â‰¥5bpm vs baseline ({round(rhr_baseline_7d, 1)})",
                        "context": f"RHR {round(latest_rhr, 1)} is {round(rhr_change, 1)}bpm above baseline, persisting {rhr_high_days} days.",
                        "persistence_days": rhr_high_days,
                        "tier": 1
                    })
                else:
                    alerts.append({
                        "metric": "rhr",
                        "value": round(latest_rhr, 1),
                        "severity": "warning",
                        "threshold": f"â†‘â‰¥5bpm vs baseline ({round(rhr_baseline_7d, 1)})",
                        "context": f"RHR {round(latest_rhr, 1)} is {round(rhr_change, 1)}bpm above baseline. Monitor â€” alarm if persists >2 days.",
                        "persistence_days": rhr_high_days,
                        "tier": 1
                    })
        
        # --- Durability Alerts (v3.4.0) ---
        # Aggregate decoupling trend from capability metrics
        capability = derived_metrics.get("capability", {})
        durability = capability.get("durability", {})
        dur_mean_7d = durability.get("mean_decoupling_7d")
        dur_mean_28d = durability.get("mean_decoupling_28d")
        dur_trend = durability.get("trend")
        dur_high_drift_7d = durability.get("high_drift_count_7d", 0)

        # Alarm: sustained high decoupling (28d mean > 5%)
        if dur_mean_28d is not None and dur_mean_28d > 5.0:
            alerts.append({
                "metric": "durability",
                "value": dur_mean_28d,
                "severity": "alarm",
                "threshold": "28d mean > 5%",
                "context": f"Sustained high decoupling ({dur_mean_28d}% 28d mean). Aerobic efficiency concern â€” review volume and recovery.",
                "persistence_days": None,
                "tier": 3
            })
        # Warning: declining trend with >2% delta
        elif (dur_trend == "declining" and dur_mean_7d is not None
              and dur_mean_28d is not None
              and (dur_mean_7d - dur_mean_28d) > 2.0):
            alerts.append({
                "metric": "durability",
                "value": dur_mean_7d,
                "severity": "warning",
                "threshold": "7d > 28d by > 2%",
                "context": f"Durability declining: 7d mean decoupling {dur_mean_7d}% vs 28d {dur_mean_28d}%. Check fatigue and recovery.",
                "persistence_days": None,
                "tier": 3
            })

        # Warning: repeated poor durability (>= 3 high-drift sessions in 7d)
        if dur_high_drift_7d >= 3:
            alerts.append({
                "metric": "durability",
                "value": dur_high_drift_7d,
                "severity": "warning",
                "threshold": ">= 3 sessions with >5% decoupling in 7d",
                "context": f"Repeated poor durability: {dur_high_drift_7d} sessions with >5% decoupling in last 7 days.",
                "persistence_days": None,
                "tier": 3
            })

        # --- TID Drift Alerts (v3.4.0) ---
        tid_comparison = capability.get("tid_comparison", {})
        tid_drift = tid_comparison.get("drift")

        if tid_drift == "acute_depolarization":
            pi_7d = tid_comparison.get("pi_7d")
            pi_28d = tid_comparison.get("pi_28d")
            alerts.append({
                "metric": "tid_distribution",
                "value": pi_7d,
                "severity": "warning",
                "threshold": "7d PI < 2.0, 28d PI >= 2.0",
                "context": f"Acute depolarization: 7d PI {pi_7d} vs 28d PI {pi_28d}. Grey zone or threshold work displacing polarized structure.",
                "persistence_days": None,
                "tier": 3
            })
        elif tid_drift == "shifting":
            cls_7d = tid_comparison.get("classification_7d")
            cls_28d = tid_comparison.get("classification_28d")
            alerts.append({
                "metric": "tid_distribution",
                "value": cls_7d,
                "severity": "warning",
                "threshold": "7d/28d classification mismatch",
                "context": f"TID shift: 7d {cls_7d} vs 28d {cls_28d}. Training distribution changing.",
                "persistence_days": None,
                "tier": 3
            })
        
        # Sort by tier (lower = more important), then severity
        severity_order = {"alarm": 0, "warning": 1, "info": 2}
        alerts.sort(key=lambda a: (a["tier"], severity_order.get(a["severity"], 3)))
        
        return alerts
    
    def _detect_deload_context(self, tss_7d_total: float, tss_28d_total: float) -> Optional[str]:
        """
        Detect if current period is a deload or post-deload transition.
        
        A deload is detected when trailing 7-day TSS is â‰¥20% below the 28-day weekly average.
        Returns context string if deload detected, None otherwise.
        """
        if not tss_28d_total or tss_28d_total == 0:
            return None
        
        weekly_avg_28d = tss_28d_total / 4  # 4 weeks
        
        if weekly_avg_28d == 0:
            return None
        
        deficit_pct = ((weekly_avg_28d - tss_7d_total) / weekly_avg_28d) * 100
        
        if deficit_pct >= 20:
            return f"deload pattern detected (7-day TSS {round(tss_7d_total)} is {round(deficit_pct)}% below 28-day weekly avg {round(weekly_avg_28d)})"
        
        return None

    @staticmethod
    def _is_valid_hrv(value: float) -> bool:
        """
        Check if HRV value is within valid physiological range (10-250ms RMSSD).
        Filters sensor errors while preserving legitimate high values in elite athletes.
        """
        return value is not None and 10 <= value <= 250

    def _count_hrv_low_days(self, wellness_7d: List[Dict], baseline: float) -> int:
        """Count consecutive days (from most recent) where HRV is â†“>20% below baseline"""
        threshold = baseline * 0.8
        count = 0
        for w in reversed(wellness_7d):
            hrv = w.get("hrv")
            if self._is_valid_hrv(hrv) and hrv < threshold:
                count += 1
            else:
                break
        return count
    
    def _count_rhr_high_days(self, wellness_7d: List[Dict], baseline: float) -> int:
        """Count consecutive days (from most recent) where RHR is â†‘â‰¥5bpm above baseline"""
        threshold = baseline + 5
        count = 0
        for w in reversed(wellness_7d):
            rhr = w.get("restingHR")
            if rhr is not None and rhr >= threshold:
                count += 1
            else:
                break
        return count
    
    # === HISTORY GENERATION (v3.3.0) ===
    
    def _get_history_confidence(self) -> Dict:
        """
        Check history.json availability and return confidence metadata.
        """
        history_path = self.script_dir / self.HISTORY_FILE
        
        if history_path.exists():
            try:
                with open(history_path, 'r') as f:
                    history_data = json.load(f)
                generated_at = history_data.get("generated_at", "")
                
                # Calculate age
                try:
                    gen_date = datetime.fromisoformat(generated_at.replace("Z", "+00:00"))
                    age_days = (datetime.now() - gen_date.replace(tzinfo=None)).days
                except:
                    age_days = None
                
                # Determine confidence from data range
                total_months = history_data.get("data_range", {}).get("total_months", 0)
                if total_months >= 12:
                    confidence = "high"
                elif total_months >= 3:
                    confidence = "medium"
                else:
                    confidence = "low"
                
                return {
                    "available": True,
                    "last_generated": generated_at[:10] if generated_at else None,
                    "age_days": age_days,
                    "total_months": total_months,
                    "history_confidence": confidence
                }
            except Exception as e:
                if self.debug:
                    print(f"  Could not read history.json: {e}")
        
        return {
            "available": False,
            "history_confidence": "low",
            "note": "No history.json available. Longitudinal analysis limited to current 28-day window."
        }
    
    def should_generate_history(self) -> bool:
        """
        Determine if history.json needs to be (re)generated.
        
        Triggers:
        - history.json missing â†’ ALWAYS generate (bypass time gate, first-run scenario)
        - history.json >28 days old â†’ regenerate (time-gated to Sun/Mon midnight)
        
        Refresh runs only on Sundays (6) or Mondays (0), in the first two runs
        after midnight (00:00 and 00:15 UTC).
        """
        history_path = self.script_dir / self.HISTORY_FILE
        
        # If history.json doesn't exist, ALWAYS generate (bypass time gate)
        if not history_path.exists():
            if self.debug:
                print("  history.json missing â€” will generate (first run)")
            return True
        
        # For REFRESH of existing history, apply the time gate
        now = datetime.now()
        
        # Only on Sundays (6) or Mondays (0)
        if now.weekday() not in [0, 6]:
            return False
        
        # Only in the first two runs after midnight (00:00-00:30)
        if now.hour > 0 or (now.hour == 0 and now.minute > 30):
            return False
        
        # Check age of existing file
        try:
            with open(history_path, 'r') as f:
                history_data = json.load(f)
            generated_at = history_data.get("generated_at", "")
            gen_date = datetime.fromisoformat(generated_at.replace("Z", "+00:00"))
            age_days = (datetime.now() - gen_date.replace(tzinfo=None)).days
            
            if age_days > 28:
                if self.debug:
                    print(f"  history.json is {age_days} days old â€” will regenerate")
                return True
            else:
                if self.debug:
                    print(f"  history.json is {age_days} days old â€” fresh enough")
                return False
        except Exception as e:
            if self.debug:
                print(f"  Could not parse history.json age: {e} â€” will regenerate")
            return True
    
    def generate_history(self) -> Dict:
        """
        Generate history.json with tiered granularity.
        
        Pulls fresh from Intervals.icu API:
        - 90-day tier: daily rows (15 fields)
        - 180-day tier: weekly aggregates (18 fields)
        - 1/2/3-year tiers: monthly aggregates (17 fields)
        - FTP timeline from API
        - Data gaps flagged factually
        """
        print("\nðŸ“Š Generating history.json...")
        
        now = datetime.now()
        
        # Determine how far back we can go (up to 3 years)
        earliest_3y = (now - timedelta(days=365 * 3)).strftime("%Y-%m-%d")
        newest = now.strftime("%Y-%m-%d")
        
        # Fetch all activities for full range
        print("  Fetching full activity history (up to 3 years)...")
        try:
            all_activities = self._intervals_get("activities", {
                "oldest": earliest_3y, "newest": newest
            })
        except Exception as e:
            print(f"  âš ï¸ Could not fetch full history: {e}")
            all_activities = []
        
        # Fetch all wellness for full range
        print("  Fetching full wellness history...")
        try:
            all_wellness = self._intervals_get("wellness", {
                "oldest": earliest_3y, "newest": newest
            })
        except Exception as e:
            print(f"  âš ï¸ Could not fetch wellness history: {e}")
            all_wellness = []
        
        # Fetch athlete data for FTP history from API
        print("  Fetching athlete settings...")
        athlete = self._intervals_get("")
        
        # Determine actual data range
        activity_dates = sorted([a.get("start_date_local", "")[:10] for a in all_activities if a.get("start_date_local")])
        
        if activity_dates:
            earliest_date = activity_dates[0]
            latest_date = activity_dates[-1]
        else:
            earliest_date = newest
            latest_date = newest
        
        try:
            earliest_dt = datetime.strptime(earliest_date, "%Y-%m-%d")
            total_months = max(1, int((now - earliest_dt).days / 30.44))
        except:
            total_months = 0
        
        # Build wellness lookup by date
        wellness_by_date = {}
        for w in all_wellness:
            date_str = w.get("id", "")
            if date_str:
                wellness_by_date[date_str] = w
        
        # Build activity lookup by date
        activities_by_date = defaultdict(list)
        for a in all_activities:
            date_str = a.get("start_date_local", "")[:10]
            if date_str:
                activities_by_date[date_str].append(a)
        
        # === FTP TIMELINE (from wellness sportInfo history or settings) ===
        ftp_timeline = self._build_ftp_timeline(all_wellness, athlete)
        
        # === DATA GAPS ===
        data_gaps = self._find_data_gaps(activity_dates, earliest_date, latest_date)
        
        # === 90-DAY DAILY ===
        print("  Building 90-day daily tier...")
        daily_90d = self._build_daily_tier(activities_by_date, wellness_by_date, days=90)
        
        # === 180-DAY WEEKLY ===
        print("  Building 180-day weekly tier...")
        weekly_180d = self._build_weekly_tier(activities_by_date, wellness_by_date, days=180)
        
        # === MONTHLY TIERS ===
        monthly_tiers = {}
        for years in [1, 2, 3]:
            label = f"{years}y"
            days_back = years * 365
            if total_months >= years * 12 * 0.5:  # Only generate if enough data
                print(f"  Building {label} monthly tier...")
                monthly_tiers[f"monthly_{label}"] = self._build_monthly_tier(
                    activities_by_date, wellness_by_date, days=days_back
                )
            else:
                monthly_tiers[f"monthly_{label}"] = []
        
        # === SUMMARIES ===
        summaries = self._build_history_summaries(daily_90d, weekly_180d, monthly_tiers)
        
        history = {
            "generated_at": now.isoformat(),
            "source": "Intervals.icu API",
            "sync_version": self.VERSION,
            "data_range": {
                "earliest": earliest_date,
                "latest": latest_date,
                "total_months": total_months
            },
            "ftp_timeline": ftp_timeline,
            "data_gaps": data_gaps,
            "summaries": summaries,
            "daily_90d": daily_90d,
            "weekly_180d": weekly_180d,
            **monthly_tiers
        }
        
        # Save locally
        history_path = self.script_dir / self.HISTORY_FILE
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2, default=str)
        print(f"  âœ… history.json saved ({len(daily_90d)} daily, {len(weekly_180d)} weekly rows)")
        
        return history
    
    def _build_daily_tier(self, activities_by_date: Dict, wellness_by_date: Dict, 
                          days: int) -> List[Dict]:
        """Build daily resolution rows for the 90-day tier."""
        rows = []
        now = datetime.now()
        
        for i in range(days - 1, -1, -1):
            date = (now - timedelta(days=i))
            date_str = date.strftime("%Y-%m-%d")
            
            day_activities = activities_by_date.get(date_str, [])
            wellness = wellness_by_date.get(date_str, {})
            
            total_tss = sum(a.get("icu_training_load", 0) or 0 for a in day_activities)
            total_seconds = sum(a.get("moving_time", 0) or 0 for a in day_activities)
            activity_types = list(set(a.get("type", "Unknown") for a in day_activities)) if day_activities else ["Rest"]
            
            # Zone ladder for hard day detection
            # Cumulative thresholds: z3+ / z4+ / z5+ / z6+ / z7
            # Per Seiler's polarized model + Foster's session RPE scaling
            day_z3 = 0
            day_z4 = 0
            day_z5 = 0
            day_z6 = 0
            day_z7 = 0
            for a in day_activities:
                icu_zone_times = a.get("icu_zone_times", [])
                if icu_zone_times:
                    for zone in icu_zone_times:
                        zid = zone.get("id", "").lower()
                        secs = zone.get("secs", 0)
                        if zid == "z3":
                            day_z3 += secs
                        elif zid == "z4":
                            day_z4 += secs
                        elif zid == "z5":
                            day_z5 += secs
                        elif zid == "z6":
                            day_z6 += secs
                        elif zid == "z7":
                            day_z7 += secs
            is_hard = (
                (day_z3 + day_z4 + day_z5 + day_z6 + day_z7) >= 1800 or
                (day_z4 + day_z5 + day_z6 + day_z7) >= 600 or
                (day_z5 + day_z6 + day_z7) >= 300 or
                (day_z6 + day_z7) >= 120 or
                day_z7 >= 60
            )
            
            rows.append({
                "date": date_str,
                "total_hours": round(total_seconds / 3600, 2),
                "total_tss": round(total_tss, 0),
                "activity_count": len(day_activities),
                "activity_types": ", ".join(activity_types),
                "ctl": wellness.get("ctl"),
                "atl": wellness.get("atl"),
                "tsb": round(wellness.get("ctl", 0) - wellness.get("atl", 0), 1) if wellness.get("ctl") and wellness.get("atl") else None,
                "hrv": wellness.get("hrv"),
                "rhr": wellness.get("restingHR"),
                "sleep_hours": round(wellness.get("sleepSecs", 0) / 3600, 2) if wellness.get("sleepSecs") else None,
                "sleep_quality": wellness.get("sleepQuality"),
                "feel": None,  # Not available in wellness, only in activities
                "weight_kg": wellness.get("weight"),
                "is_hard_day": is_hard
            })
            
            # Check feel from activities
            for a in day_activities:
                feel = a.get("feel")
                if feel:
                    rows[-1]["feel"] = feel
                    break
        
        return rows
    
    def _build_weekly_tier(self, activities_by_date: Dict, wellness_by_date: Dict,
                           days: int) -> List[Dict]:
        """Build weekly aggregate rows for the 180-day tier."""
        rows = []
        now = datetime.now()
        
        # Calculate weeks
        start_date = now - timedelta(days=days)
        # Align to Monday
        start_monday = start_date - timedelta(days=start_date.weekday())
        
        current = start_monday
        while current < now:
            week_end = current + timedelta(days=6)
            if week_end > now:
                week_end = now
            
            week_tss = 0
            week_seconds = 0
            week_activities = 0
            week_hrv = []
            week_rhr = []
            week_sleep = []
            week_feel = []
            week_weight = []
            hard_days = 0
            longest_ride = 0
            z1_z2_time = 0
            z3_time = 0
            z4_plus_time = 0
            total_zone_time = 0
            ctl_end = None
            atl_end = None
            tsb_end = None
            ramp_rate = None
            
            for d in range(7):
                date = current + timedelta(days=d)
                if date > now:
                    break
                date_str = date.strftime("%Y-%m-%d")
                
                day_activities = activities_by_date.get(date_str, [])
                wellness = wellness_by_date.get(date_str, {})
                
                day_tss = sum(a.get("icu_training_load", 0) or 0 for a in day_activities)
                day_seconds = sum(a.get("moving_time", 0) or 0 for a in day_activities)
                
                week_tss += day_tss
                week_seconds += day_seconds
                week_activities += len(day_activities)

                if self._is_valid_hrv(wellness.get("hrv")):
                    week_hrv.append(wellness["hrv"])
                if wellness.get("restingHR"):
                    week_rhr.append(wellness["restingHR"])
                if wellness.get("sleepSecs"):
                    week_sleep.append(wellness["sleepSecs"] / 3600)
                if wellness.get("weight"):
                    week_weight.append(wellness["weight"])
                
                ctl_end = wellness.get("ctl") or ctl_end
                atl_end = wellness.get("atl") or atl_end
                ramp_rate = wellness.get("rampRate") or ramp_rate
                
                # Zone and hard day analysis
                day_z3 = 0
                day_z4 = 0
                day_z5 = 0
                day_z6 = 0
                day_z7 = 0
                for a in day_activities:
                    ride_seconds = a.get("moving_time", 0) or 0
                    if ride_seconds > longest_ride:
                        longest_ride = ride_seconds
                    
                    icu_zone_times = a.get("icu_zone_times", [])
                    if icu_zone_times:
                        for zone in icu_zone_times:
                            zid = zone.get("id", "").lower()
                            secs = zone.get("secs", 0)
                            if zid in ["z1", "z2"]:
                                z1_z2_time += secs
                            elif zid == "z3":
                                z3_time += secs
                                day_z3 += secs
                            elif zid == "z4":
                                z4_plus_time += secs
                                day_z4 += secs
                            elif zid == "z5":
                                z4_plus_time += secs
                                day_z5 += secs
                            elif zid == "z6":
                                z4_plus_time += secs
                                day_z6 += secs
                            elif zid == "z7":
                                z4_plus_time += secs
                                day_z7 += secs
                            total_zone_time += secs
                    
                    feel = a.get("feel")
                    if feel:
                        week_feel.append(feel)
                
                if (
                    (day_z3 + day_z4 + day_z5 + day_z6 + day_z7) >= 1800 or
                    (day_z4 + day_z5 + day_z6 + day_z7) >= 600 or
                    (day_z5 + day_z6 + day_z7) >= 300 or
                    (day_z6 + day_z7) >= 120 or
                    day_z7 >= 60
                ):
                    hard_days += 1
            
            if ctl_end and atl_end:
                tsb_end = round(ctl_end - atl_end, 1)
            
            rows.append({
                "week_start": current.strftime("%Y-%m-%d"),
                "total_hours": round(week_seconds / 3600, 2),
                "total_tss": round(week_tss, 0),
                "activity_count": week_activities,
                "ctl_end": round(ctl_end, 1) if ctl_end else None,
                "atl_end": round(atl_end, 1) if atl_end else None,
                "tsb_end": tsb_end,
                "ramp_rate": round(ramp_rate, 2) if ramp_rate else None,
                "avg_hrv": round(statistics.mean(week_hrv), 1) if week_hrv else None,
                "avg_rhr": round(statistics.mean(week_rhr), 1) if week_rhr else None,
                "avg_sleep_hours": round(statistics.mean(week_sleep), 2) if week_sleep else None,
                "z1_z2_pct": round((z1_z2_time / total_zone_time) * 100, 1) if total_zone_time > 0 else None,
                "z3_pct": round((z3_time / total_zone_time) * 100, 1) if total_zone_time > 0 else None,
                "z4_plus_pct": round((z4_plus_time / total_zone_time) * 100, 1) if total_zone_time > 0 else None,
                "hard_days": hard_days,
                "longest_ride_hours": round(longest_ride / 3600, 2),
                "avg_feel": round(statistics.mean(week_feel), 1) if week_feel else None,
                "weight_kg": round(week_weight[-1], 1) if week_weight else None
            })
            
            current += timedelta(days=7)
        
        return rows
    
    def _build_monthly_tier(self, activities_by_date: Dict, wellness_by_date: Dict,
                            days: int) -> List[Dict]:
        """Build monthly aggregate rows for 1/2/3-year tiers."""
        rows = []
        now = datetime.now()
        start_date = now - timedelta(days=days)
        
        # Group by month
        current_month = datetime(start_date.year, start_date.month, 1)
        
        while current_month <= now:
            month_str = current_month.strftime("%Y-%m")
            
            # Determine days in this month
            if current_month.month == 12:
                next_month = datetime(current_month.year + 1, 1, 1)
            else:
                next_month = datetime(current_month.year, current_month.month + 1, 1)
            
            month_tss = 0
            month_seconds = 0
            month_activities = 0
            month_hrv = []
            month_rhr = []
            month_weight = []
            ctl_values = []
            hard_days_total = 0
            longest_ride = 0
            z1_z2_time = 0
            z3_time = 0
            z4_plus_time = 0
            total_zone_time = 0
            days_with_data = 0
            total_days_in_month = 0
            
            date = current_month
            while date < next_month and date <= now:
                date_str = date.strftime("%Y-%m-%d")
                total_days_in_month += 1
                
                day_activities = activities_by_date.get(date_str, [])
                wellness = wellness_by_date.get(date_str, {})
                
                if day_activities or wellness:
                    days_with_data += 1
                
                day_tss = sum(a.get("icu_training_load", 0) or 0 for a in day_activities)
                day_seconds = sum(a.get("moving_time", 0) or 0 for a in day_activities)
                
                month_tss += day_tss
                month_seconds += day_seconds
                month_activities += len(day_activities)


                if self._is_valid_hrv(wellness.get("hrv")):
                    month_hrv.append(wellness["hrv"])
                if wellness.get("restingHR"):
                    month_rhr.append(wellness["restingHR"])
                if wellness.get("weight"):
                    month_weight.append(wellness["weight"])
                if wellness.get("ctl"):
                    ctl_values.append(wellness["ctl"])
                
                day_z3 = 0
                day_z4 = 0
                day_z5 = 0
                day_z6 = 0
                day_z7 = 0
                for a in day_activities:
                    ride_seconds = a.get("moving_time", 0) or 0
                    if ride_seconds > longest_ride:
                        longest_ride = ride_seconds
                    
                    icu_zone_times = a.get("icu_zone_times", [])
                    if icu_zone_times:
                        for zone in icu_zone_times:
                            zid = zone.get("id", "").lower()
                            secs = zone.get("secs", 0)
                            if zid in ["z1", "z2"]:
                                z1_z2_time += secs
                            elif zid == "z3":
                                z3_time += secs
                                day_z3 += secs
                            elif zid == "z4":
                                z4_plus_time += secs
                                day_z4 += secs
                            elif zid == "z5":
                                z4_plus_time += secs
                                day_z5 += secs
                            elif zid == "z6":
                                z4_plus_time += secs
                                day_z6 += secs
                            elif zid == "z7":
                                z4_plus_time += secs
                                day_z7 += secs
                            total_zone_time += secs
                
                if (
                    (day_z3 + day_z4 + day_z5 + day_z6 + day_z7) >= 1800 or
                    (day_z4 + day_z5 + day_z6 + day_z7) >= 600 or
                    (day_z5 + day_z6 + day_z7) >= 300 or
                    (day_z6 + day_z7) >= 120 or
                    day_z7 >= 60
                ):
                    hard_days_total += 1
                
                date += timedelta(days=1)
            
            # Calculate weeks in this month for per-week averages
            weeks_in_period = max(1, total_days_in_month / 7)
            
            # Determine dominant phase (simplified: based on CTL trend and zone distribution)
            dominant_phase = "Unknown"
            if ctl_values and len(ctl_values) >= 2:
                ctl_trend = ctl_values[-1] - ctl_values[0]
                qi_pct = (z4_plus_time / total_zone_time * 100) if total_zone_time > 0 else 0
                
                if ctl_trend > 3 and qi_pct > 15:
                    dominant_phase = "Build"
                elif ctl_trend > 1:
                    dominant_phase = "Base"
                elif ctl_trend < -3:
                    dominant_phase = "Recovery"
                else:
                    dominant_phase = "Maintenance"
            
            rows.append({
                "month": month_str,
                "total_hours": round(month_seconds / 3600, 2),
                "total_tss": round(month_tss, 0),
                "activity_count": month_activities,
                "ctl_peak": round(max(ctl_values), 1) if ctl_values else None,
                "ctl_low": round(min(ctl_values), 1) if ctl_values else None,
                "ctl_end": round(ctl_values[-1], 1) if ctl_values else None,
                "avg_hrv": round(statistics.mean(month_hrv), 1) if month_hrv else None,
                "avg_rhr": round(statistics.mean(month_rhr), 1) if month_rhr else None,
                "z1_z2_pct": round((z1_z2_time / total_zone_time) * 100, 1) if total_zone_time > 0 else None,
                "z3_pct": round((z3_time / total_zone_time) * 100, 1) if total_zone_time > 0 else None,
                "z4_plus_pct": round((z4_plus_time / total_zone_time) * 100, 1) if total_zone_time > 0 else None,
                "hard_days_avg_per_week": round(hard_days_total / weeks_in_period, 1),
                "longest_ride_hours": round(longest_ride / 3600, 2),
                "avg_weight_kg": round(statistics.mean(month_weight), 1) if month_weight else None,
                "dominant_phase": dominant_phase,
                "days_with_data": days_with_data
            })
            
            current_month = next_month
        
        return rows
    
    def _build_ftp_timeline(self, all_wellness: List[Dict], athlete: Dict) -> List[Dict]:
        """
        Build FTP timeline from ftp_history.json (actual user-set FTP values).
        Falls back to current sportSettings if no history file exists.
        """
        timeline = []
        
        # Primary source: ftp_history.json (tracked by sync.py on each run)
        ftp_history = self._load_ftp_history()
        
        for ftp_type in ["indoor", "outdoor"]:
            entries = ftp_history.get(ftp_type, {})
            for date_str, ftp_val in sorted(entries.items()):
                timeline.append({
                    "date": date_str,
                    "ftp": ftp_val,
                    "type": ftp_type,
                    "source": "FTP"
                })
        
        # Fallback: add current user-set FTP if not already in timeline
        cycling_settings = None
        if athlete.get("sportSettings"):
            for sport in athlete["sportSettings"]:
                if "Ride" in sport.get("types", []) or "VirtualRide" in sport.get("types", []):
                    cycling_settings = sport
                    break
        
        if cycling_settings:
            today = datetime.now().strftime("%Y-%m-%d")
            outdoor_ftp = cycling_settings.get("ftp")
            indoor_ftp = cycling_settings.get("indoor_ftp")
            
            # Check if current FTP is already the latest in timeline
            outdoor_dates = {e["date"]: e["ftp"] for e in timeline if e["type"] == "outdoor"}
            indoor_dates = {e["date"]: e["ftp"] for e in timeline if e["type"] == "indoor"}
            
            latest_outdoor = outdoor_dates.get(max(outdoor_dates.keys())) if outdoor_dates else None
            latest_indoor = indoor_dates.get(max(indoor_dates.keys())) if indoor_dates else None
            
            if outdoor_ftp and outdoor_ftp != latest_outdoor:
                timeline.append({"date": today, "ftp": outdoor_ftp, "type": "outdoor", "source": "user_set"})
            if indoor_ftp and indoor_ftp != latest_indoor:
                timeline.append({"date": today, "ftp": indoor_ftp, "type": "indoor", "source": "user_set"})
        
        # Sort chronologically
        timeline.sort(key=lambda x: (x["date"], x["type"]))
        
        return timeline
    
    def _find_data_gaps(self, activity_dates: List[str], earliest: str, latest: str) -> List[Dict]:
        """
        Find periods with no activity data (gaps â‰¥ 3 days).
        Flags factually without inference about reasons.
        """
        gaps = []
        if not activity_dates:
            return gaps
        
        date_set = set(activity_dates)
        
        try:
            start = datetime.strptime(earliest, "%Y-%m-%d")
            end = datetime.strptime(latest, "%Y-%m-%d")
        except:
            return gaps
        
        gap_start = None
        current = start
        
        while current <= end:
            date_str = current.strftime("%Y-%m-%d")
            
            if date_str not in date_set:
                if gap_start is None:
                    gap_start = current
            else:
                if gap_start is not None:
                    gap_days = (current - gap_start).days
                    if gap_days >= 3:
                        gaps.append({
                            "period": f"{gap_start.strftime('%Y-%m-%d')} to {(current - timedelta(days=1)).strftime('%Y-%m-%d')}",
                            "days_missing": gap_days
                        })
                    gap_start = None
            
            current += timedelta(days=1)
        
        # Handle trailing gap
        if gap_start is not None:
            gap_days = (end - gap_start).days + 1
            if gap_days >= 3:
                gaps.append({
                    "period": f"{gap_start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}",
                    "days_missing": gap_days
                })
        
        return gaps
    
    def _build_history_summaries(self, daily_90d: List[Dict], weekly_180d: List[Dict],
                                  monthly_tiers: Dict) -> Dict:
        """Build pre-computed summaries for each tier."""
        summaries = {}
        
        # 90-day summary from daily data
        if daily_90d:
            tss_values = [d["total_tss"] for d in daily_90d if d["total_tss"]]
            hours_values = [d["total_hours"] for d in daily_90d if d["total_hours"]]
            ctl_values = [d["ctl"] for d in daily_90d if d["ctl"]]
            
            summaries["90d"] = {
                "avg_weekly_tss": round(sum(tss_values) / max(1, len(daily_90d) / 7), 0) if tss_values else None,
                "avg_weekly_hours": round(sum(hours_values) / max(1, len(daily_90d) / 7), 1) if hours_values else None,
                "ctl_start": round(ctl_values[0], 1) if ctl_values else None,
                "ctl_end": round(ctl_values[-1], 1) if ctl_values else None,
                "total_activities": sum(1 for d in daily_90d if d["activity_count"] > 0),
                "rest_days": sum(1 for d in daily_90d if d["activity_count"] == 0),
                "hard_days": sum(1 for d in daily_90d if d.get("is_hard_day"))
            }
        
        # 180-day summary from weekly data
        if weekly_180d:
            tss_values = [w["total_tss"] for w in weekly_180d if w["total_tss"]]
            hours_values = [w["total_hours"] for w in weekly_180d if w["total_hours"]]
            ctl_values = [w["ctl_end"] for w in weekly_180d if w["ctl_end"]]
            
            summaries["180d"] = {
                "avg_weekly_tss": round(statistics.mean(tss_values), 0) if tss_values else None,
                "avg_weekly_hours": round(statistics.mean(hours_values), 1) if hours_values else None,
                "ctl_start": round(ctl_values[0], 1) if ctl_values else None,
                "ctl_end": round(ctl_values[-1], 1) if ctl_values else None,
                "weeks_tracked": len(weekly_180d)
            }
        
        # Yearly summaries from monthly data
        for key in ["monthly_1y", "monthly_2y", "monthly_3y"]:
            monthly = monthly_tiers.get(key, [])
            if monthly:
                tss_values = [m["total_tss"] for m in monthly if m["total_tss"]]
                ctl_values = [m["ctl_end"] for m in monthly if m["ctl_end"]]
                
                label = key.replace("monthly_", "")
                summaries[label] = {
                    "avg_monthly_tss": round(statistics.mean(tss_values), 0) if tss_values else None,
                    "ctl_peak": round(max(ctl_values), 1) if ctl_values else None,
                    "ctl_low": round(min(ctl_values), 1) if ctl_values else None,
                    "months_tracked": len(monthly)
                }
        
        return summaries
    
    # === UPDATE NOTIFICATIONS (v3.3.0) ===
    
    def check_upstream_updates(self):
        """
        Check CrankAddict/section-11 for new releases and create a GitHub Issue
        if there's a new notification_id.
        
        Uses date-based changelog format:
        {
            "notification_id": "2026-02-11",
            "changes": [
                "SECTION_11.md - UPDATE - 2026-02-11 - Description",
                "sync.py - UPDATE - 2026-02-11 - Description"
            ]
        }
        """
        if not self.github_token or not self.github_repo:
            if self.debug:
                print("  Skipping update check â€” no GitHub credentials")
            return
        
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github+json"
        }
        
        # Fetch changelog.json from upstream
        try:
            url = f"https://raw.githubusercontent.com/{self.UPSTREAM_REPO}/main/{self.CHANGELOG_FILE}"
            response = requests.get(url, timeout=10)
            if response.status_code != 200:
                if self.debug:
                    print(f"  No changelog.json found upstream (HTTP {response.status_code})")
                return
            
            changelog = response.json()
        except Exception as e:
            if self.debug:
                print(f"  Could not fetch upstream changelog: {e}")
            return
        
        notification_id = changelog.get("notification_id")
        if not notification_id:
            if self.debug:
                print("  No notification_id in changelog")
            return
        
        issue_title = f"Section 11 updates â€” {notification_id}"
        
        # Check if issue already exists (open or closed)
        try:
            search_url = f"{self.GITHUB_API_URL}/search/issues"
            search_params = {
                "q": f'repo:{self.github_repo} "{issue_title}" in:title'
            }
            response = requests.get(search_url, headers=headers, params=search_params, timeout=10)
            
            if response.status_code == 200:
                results = response.json()
                if results.get("total_count", 0) > 0:
                    if self.debug:
                        print(f"  Update notification already exists: {issue_title}")
                    return
        except Exception as e:
            if self.debug:
                print(f"  Could not search issues: {e}")
            return
        
        # Create new issue
        changes = changelog.get("changes", [])
        body = f"## Section 11 Update Available\n\n"
        body += f"**Notification ID:** {notification_id}\n\n"
        body += "### Changes:\n"
        for change in changes:
            body += f"- {change}\n"
        body += f"\n### Repository:\n"
        body += f"https://github.com/{self.UPSTREAM_REPO}\n"
        body += f"\n*This issue was auto-created by sync.py v{self.VERSION}*"
        
        try:
            issues_url = f"{self.GITHUB_API_URL}/repos/{self.github_repo}/issues"
            payload = {
                "title": issue_title,
                "body": body,
                "labels": ["update-notification"]
            }
            response = requests.post(issues_url, headers=headers, json=payload, timeout=10)
            
            if response.status_code == 201:
                print(f"  ðŸ“¢ Update notification created: {issue_title}")
            else:
                if self.debug:
                    print(f"  Could not create issue (HTTP {response.status_code}): {response.text}")
        except Exception as e:
            if self.debug:
                print(f"  Could not create update issue: {e}")
    
    def _format_activities(self, activities: List[Dict], anonymize: bool = False) -> List[Dict]:
        """Format activities for LLM analysis"""
        formatted = []
        for i, act in enumerate(activities):
            avg_power = (act.get("average_watts") or act.get("avg_watts") or 
                        act.get("average_power") or act.get("avgWatts") or
                        act.get("icu_average_watts"))
            norm_power = (act.get("weighted_average_watts") or act.get("np") or 
                         act.get("icu_pm_np") or act.get("normalizedPower") or
                         act.get("icu_weighted_avg_watts"))
            avg_hr = (act.get("average_heartrate") or act.get("avg_hr") or 
                     act.get("average_heart_rate") or act.get("avgHr") or
                     act.get("icu_average_hr"))
            max_hr = (act.get("max_heartrate") or act.get("max_hr") or 
                     act.get("max_heart_rate") or act.get("maxHr") or
                     act.get("icu_max_hr"))
            
            avg_cadence = (act.get("average_cadence") or act.get("avg_cadence") or
                          act.get("icu_average_cadence"))
            avg_temp = (act.get("average_weather_temp") or act.get("average_temp") or 
                       act.get("avg_temp") or act.get("average_temperature"))
            joules = act.get("icu_joules")
            work_kj = round(joules / 1000, 1) if joules else None
            calories = act.get("calories") or act.get("icu_calories")
            variability_index = act.get("icu_variability_index")
            decoupling = act.get("icu_hr_decoupling") or act.get("decoupling")
            
            avg_speed_ms = act.get("average_speed")
            max_speed_ms = act.get("max_speed")
            avg_speed = round(avg_speed_ms * 3.6, 1) if avg_speed_ms else None
            max_speed = round(max_speed_ms * 3.6, 1) if max_speed_ms else None
            avg_pace = act.get("average_pace") or act.get("icu_pace")
            
            weather = act.get("weather_description") or act.get("weather")
            humidity = act.get("humidity") or act.get("average_humidity")
            wind_speed = act.get("average_wind_speed") or act.get("wind_speed")
            
            carbs_used = act.get("carbs_used")
            carbs_ingested = act.get("carbs_ingested")
            
            hr_zones = {}
            power_zones = {}
            
            icu_hr_zone_times = act.get("icu_hr_zone_times", [])
            if icu_hr_zone_times and isinstance(icu_hr_zone_times, list):
                zone_labels = ["z1_time", "z2_time", "z3_time", "z4_time", "z5_time", "z6_time", "z7_time"]
                for idx, secs in enumerate(icu_hr_zone_times):
                    if idx < len(zone_labels):
                        hr_zones[zone_labels[idx]] = secs if secs is not None else 0
            
            icu_zone_times = act.get("icu_zone_times", [])
            if icu_zone_times:
                for zone in icu_zone_times:
                    zone_id = zone.get("id", "").lower()
                    secs = zone.get("secs", 0)
                    if zone_id in ["z1", "z2", "z3", "z4", "z5", "z6", "z7"]:
                        power_zones[f"{zone_id}_time"] = secs if secs is not None else 0
            
            zone_dist = {}
            if hr_zones:
                zone_dist["hr_zones"] = hr_zones
            if power_zones:
                zone_dist["power_zones"] = power_zones
            
            if not zone_dist:
                zone_dist = None
            
            activity_name = act.get("name", "")
            if anonymize:
                if act.get("type", "") in self.OUTDOOR_TYPES:
                    activity_name = "Training Session"
            
            activity = {
                "id": f"activity_{i+1}" if anonymize else act.get("id", f"unknown_{i+1}"),
                "date": act.get("start_date_local", "unknown"),
                "type": act.get("type", "Unknown"),
                "name": activity_name,
                "duration_hours": round((act.get("moving_time") or 0) / 3600, 2),
                "distance_km": round((act.get("distance") or 0) / 1000, 2),
                "tss": act.get("icu_training_load"),
                "intensity_factor": act.get("icu_intensity"),
                "avg_power": avg_power,
                "normalized_power": norm_power,
                "avg_hr": avg_hr,
                "max_hr": max_hr,
                "avg_cadence": avg_cadence,
                "avg_speed": avg_speed,
                "max_speed": max_speed,
                "avg_pace": avg_pace,
                "avg_temp": avg_temp,
                "weather": weather,
                "humidity": humidity,
                "wind_speed": wind_speed,
                "work_kj": work_kj,
                "calories": calories,
                "carbs_used": carbs_used,
                "carbs_ingested": carbs_ingested,
                "variability_index": variability_index,
                "decoupling": decoupling,
                "elevation_m": act.get("total_elevation_gain"),
                "feel": act.get("feel"),
                "rpe": act.get("icu_rpe"),
                "zone_distribution": zone_dist
            }
            
            formatted.append(activity)
        
        return formatted
    
    def _format_wellness(self, wellness: List[Dict]) -> List[Dict]:
        """Format wellness data"""
        formatted = []
        for w in wellness:
            entry = {
                "date": w.get("id", "unknown"),
                "weight_kg": w.get("weight"),
                "resting_hr": w.get("restingHR"),
                "hrv_rmssd": w.get("hrv"),
                "hrv_sdnn": w.get("hrvSdnn"),
                "sleep_hours": round(w["sleepSecs"] / 3600, 2) if w.get("sleepSecs") else None,
                "sleep_quality": w.get("sleepQuality"),
                "sleep_score": w.get("sleepScore"),
                "mental_energy": w.get("mentalEnergy"),
                "fatigue": w.get("fatigue"),
                "soreness": w.get("soreness"),
                "avg_sleeping_hr": w.get("avgSleepingHR"),
                "vo2max": w.get("vo2max")
            }
            
            formatted.append(entry)
        
        return formatted
    
    def _format_events(self, events: List[Dict], anonymize: bool = False) -> List[Dict]:
        """Format planned workouts"""
        return [{
            "id": f"event_{i+1}" if anonymize else evt.get("id", f"unknown_{i+1}"),
            "date": evt.get("start_date_local", "unknown"),
            "name": "Planned Workout" if anonymize else evt.get("name", ""),
            "type": evt.get("category", ""),
            "description": evt.get("description", ""),
            "planned_tss": evt.get("icu_training_load"),
            "duration_hours": round(evt.get("duration", 0) / 3600, 2)
        } for i, evt in enumerate(events)]
    
    def _build_race_calendar(self, future_events: List[Dict], current_ctl: float,
                              current_atl: float, current_tsb: float,
                              activities_7d: List[Dict], today: str) -> Dict:
        """
        Build race calendar with 3-layer awareness (v3.5.0).
        
        Layer 1: All races within 90-day window (always present)
        Layer 2: Taper onset alerts when RACE_A is 8-14 days out
        Layer 3: Race-week protocol when RACE_A/B is â‰¤7 days out
        
        References: Section 11A Race-Week Protocol
        Scientific basis: Mujika & Padilla (2003), Bosquet et al. (2007), Altini (HRV)
        """
        
        today_date = datetime.strptime(today, "%Y-%m-%d").date()
        
        # Filter to race events only
        race_categories = {"RACE_A", "RACE_B", "RACE_C"}
        race_events = []
        for evt in future_events:
            cat = evt.get("category", "")
            if cat in race_categories:
                start = evt.get("start_date_local", "")[:10]
                if start:
                    try:
                        evt_date = datetime.strptime(start, "%Y-%m-%d").date()
                        days_until = (evt_date - today_date).days
                        if days_until >= 0:
                            race_events.append({
                                "name": evt.get("name", "Unnamed Race"),
                                "date": start,
                                "category": cat,
                                "type": evt.get("type", "Unknown"),
                                "days_until": days_until,
                                "moving_time_seconds": evt.get("moving_time"),
                                "distance_meters": evt.get("distance"),
                                "_raw": evt  # Keep raw for race-week building
                            })
                    except ValueError:
                        continue
        
        # Sort by date
        race_events.sort(key=lambda x: x["days_until"])
        
        # Strip _raw from public output
        all_races = [{k: v for k, v in r.items() if k != "_raw"} for r in race_events]
        
        # Next race (any priority)
        next_race = all_races[0] if all_races else None
        
        # Taper alert: RACE_A within 8-14 days
        taper_race = next((r for r in race_events if r["category"] == "RACE_A" and 8 <= r["days_until"] <= 14), None)
        taper_alert = {"active": taper_race is not None}
        if taper_race:
            taper_alert["event_name"] = taper_race["name"]
            taper_alert["event_date"] = taper_race["date"]
            taper_alert["days_until"] = taper_race["days_until"]
            taper_alert["message"] = (
                f"RACE_A '{taper_race['name']}' in {taper_race['days_until']} days. "
                f"Begin volume reduction (target 41-60% over 2 weeks). Maintain intensity. "
                f"CTL should peak now or within the next few days."
            )
        
        # Race-week: RACE_A or RACE_B within 7 days
        # If both exist, prioritise RACE_A
        race_week_candidates = [r for r in race_events if r["category"] in {"RACE_A", "RACE_B"} and r["days_until"] <= 7]
        race_week_target = None
        if race_week_candidates:
            a_races = [r for r in race_week_candidates if r["category"] == "RACE_A"]
            race_week_target = a_races[0] if a_races else race_week_candidates[0]
        
        race_week = {"active": False}
        if race_week_target:
            race_week = self._build_race_week(
                race_event=race_week_target,
                current_ctl=current_ctl,
                current_atl=current_atl,
                current_tsb=current_tsb,
                activities_7d=activities_7d,
                today_date=today_date
            )
        
        return {
            "next_race": next_race,
            "all_races": all_races,
            "taper_alert": taper_alert,
            "race_week": race_week
        }
    
    def _build_race_week(self, race_event: Dict, current_ctl: float,
                          current_atl: float, current_tsb: float,
                          activities_7d: List[Dict], today_date) -> Dict:
        """
        Build race-week protocol data for D-7 through D-0.
        
        All load targets are relative to CTL. Normal weekly TSS = CTL Ã— 7.
        Race-week TSS budget: 40-55% of normal weekly TSS (RACE_A) or 50-65% (RACE_B).
        
        TSB projection uses PMC decay: CTL_decay = e^(-1/42), ATL_decay = e^(-1/7).
        Assumes zero training load for remaining days to project race-day TSB.
        """
        
        evt_date = datetime.strptime(race_event["date"], "%Y-%m-%d").date()
        days_until = race_event["days_until"]
        category = race_event["category"]
        moving_time = race_event.get("moving_time_seconds")
        
        # Current day label
        current_day = f"D-{days_until}" if days_until > 0 else "D-0"
        
        # CTL baseline and normal weekly TSS
        ctl_baseline = current_ctl if current_ctl else 0
        normal_weekly_tss = round(ctl_baseline * 7, 1)
        
        # Race-week TSS budget (relative to category)
        if category == "RACE_A":
            budget_min_pct, budget_max_pct = 0.40, 0.55
        else:  # RACE_B
            budget_min_pct, budget_max_pct = 0.50, 0.65
        
        budget_min = round(normal_weekly_tss * budget_min_pct)
        budget_max = round(normal_weekly_tss * budget_max_pct)
        
        # Race-week TSS spent: sum TSS from activities within race week window
        race_week_start = evt_date - timedelta(days=7)
        tss_spent = 0
        for act in activities_7d:
            act_date_str = act.get("start_date_local", "")[:10]
            if act_date_str:
                try:
                    act_date = datetime.strptime(act_date_str, "%Y-%m-%d").date()
                    if race_week_start <= act_date <= today_date:
                        tss_spent += act.get("icu_training_load", 0) or 0
                except ValueError:
                    continue
        tss_spent = round(tss_spent)
        
        # TSB projection for race day (assume zero load for remaining days)
        ctl_decay = math.exp(-1/42)   # ~0.9765
        atl_decay = math.exp(-1/7)    # ~0.8668
        
        proj_ctl = current_ctl if current_ctl else 0
        proj_atl = current_atl if current_atl else 0
        for _ in range(days_until):
            proj_ctl *= ctl_decay
            proj_atl *= atl_decay
        projected_tsb = round(proj_ctl - proj_atl, 1)
        
        # Event duration classification
        if moving_time is not None:
            if moving_time < 5400:
                duration_class = "short_intense"
            elif moving_time <= 10800:
                duration_class = "medium"
            else:
                duration_class = "long_endurance"
        else:
            # Default by category when not set
            duration_class = "long_endurance" if category == "RACE_A" else "medium"
        
        # TSB target range by duration class
        tsb_targets = {
            "short_intense": {"min": 5, "max": 15},
            "medium": {"min": 10, "max": 20},
            "long_endurance": {"min": 10, "max": 25}
        }
        tsb_range = tsb_targets.get(duration_class, {"min": 10, "max": 25})
        
        # RACE_B: lower TSB target by 5
        if category == "RACE_B":
            tsb_range = {"min": max(0, tsb_range["min"] - 5), "max": tsb_range["max"] - 5}
        
        # Day-by-day decision tree
        day_protocol = self._get_day_protocol(days_until, ctl_baseline, duration_class, category)
        
        # Carb loading
        carb_applicable = False
        if moving_time is not None:
            carb_applicable = moving_time >= 5400
        elif category == "RACE_A":
            carb_applicable = True  # Default assumption for A races
        
        carb_active = carb_applicable and days_until <= 4
        carb_start_date = (evt_date - timedelta(days=4)).strftime("%Y-%m-%d")
        
        # Opener day (D-2)
        opener_date = (evt_date - timedelta(days=2)).strftime("%Y-%m-%d")
        opener_intensity = "lighter" if duration_class == "long_endurance" else (
            "more_intense" if duration_class == "short_intense" else "standard"
        )
        
        # Go/no-go: TSB status
        if projected_tsb >= tsb_range["min"]:
            tsb_status = "green"
            go_notes = []
        elif projected_tsb >= tsb_range["min"] - 10:
            tsb_status = "flag"
            go_notes = [f"Projected race-day TSB {projected_tsb} is below target range {tsb_range['min']}-{tsb_range['max']}. Consider additional rest."]
        else:
            tsb_status = "flag"
            go_notes = [f"Projected race-day TSB {projected_tsb} is significantly below target range {tsb_range['min']}-{tsb_range['max']}. Fatigue may impact performance."]
        
        return {
            "active": True,
            "event_name": race_event["name"],
            "event_date": race_event["date"],
            "event_category": category,
            "event_type": race_event.get("type", "Unknown"),
            "event_duration_class": duration_class,
            "event_moving_time_seconds": moving_time,
            "days_until_event": days_until,
            "current_day": current_day,
            "ctl_baseline": round(ctl_baseline, 1),
            "normal_weekly_tss": normal_weekly_tss,
            "race_week_tss_budget": {"min": budget_min, "max": budget_max},
            "race_week_tss_spent": tss_spent,
            "race_week_tss_remaining": {
                "min": max(0, budget_min - tss_spent),
                "max": max(0, budget_max - tss_spent)
            },
            "projected_race_day_tsb": projected_tsb,
            "tsb_target_range": tsb_range,
            "today": day_protocol,
            "carb_loading": {
                "applicable": carb_applicable,
                "active": carb_active,
                "starts": "D-4",
                "start_date": carb_start_date,
                "note": "10-12 gÂ·kgâ»Â¹/day. No depletion phase needed." if carb_applicable else None
            },
            "opener": {
                "day": "D-2",
                "date": opener_date,
                "intensity": opener_intensity
            },
            "go_no_go": {
                "tsb_status": tsb_status,
                "notes": go_notes
            }
        }
    
    def _get_day_protocol(self, days_until: int, ctl: float, duration_class: str, category: str) -> Dict:
        """
        Return today's race-week protocol based on days until event.
        Load targets as TSS = percentage of CTL.
        """
        # Day protocol definitions: (label, min_pct, max_pct, zones, purpose)
        protocols = {
            7: ("Last key session", 0.75, 1.00, "3-5 efforts Z4-Z5 (1-3 min)", "Fitness confirmation. Verify strong power/HR response."),
            6: ("Recovery", 0.00, 0.30, "Z1-Z2 only", "Active recovery."),
            5: ("Moderate endurance", 0.40, 0.60, "Z1-Z2 + 2-3 race-pace touches", "Maintain feel without adding fatigue."),
            4: ("Easy / rest", 0.00, 0.40, "Z1-Z2 only", "Volume reduction. Carb loading begins if applicable."),
            3: ("Easy / rest", 0.00, 0.40, "Z1-Z2 only", "Taper tantrums expected (D-4 to D-2). Normal â€” not lost fitness."),
            2: ("Opener", 0.30, 0.50, "3-5 efforts Z4-Z6 (20-60s), high cadence, full recovery", "Neuromuscular activation."),
            1: ("Rest / minimal", 0.00, 0.20, "Z1 only if active", "Final rest, logistics, equipment check."),
            0: ("Race day", 0.00, 0.00, "Race effort", "Go/no-go assessment. Execute race plan.")
        }
        
        # Default for days > 7 (shouldn't happen in race week, but defensive)
        if days_until > 7:
            return {
                "label": "Pre-race-week",
                "load_target_tss": None,
                "zones": "Normal training",
                "purpose": "Race week protocol not yet active for this day."
            }
        
        label, min_pct, max_pct, zones, purpose = protocols.get(days_until, protocols[0])
        
        # Adjust opener intensity by duration class
        if days_until == 2:
            if duration_class == "long_endurance":
                zones = "3-4 efforts Z4 only (20-60s), moderate cadence, full recovery"
                purpose = "Light neuromuscular activation. Preserve glycogen."
            elif duration_class == "short_intense":
                zones = "5-6 efforts Z4-Z6 (10-30s), high cadence, full recovery"
                purpose = "Full neuromuscular activation for short, intense effort."
        
        # For long endurance events, prefer easy endurance over complete rest on D-4/D-3
        if days_until in (3, 4) and duration_class == "long_endurance":
            min_pct = 0.20  # Nudge minimum up â€” easy spin preferred over full rest
            purpose = f"{purpose} Easy endurance preferred over complete rest for long events."
        
        min_tss = round(ctl * min_pct)
        max_tss = round(ctl * max_pct)
        
        return {
            "label": label,
            "load_target_tss": {"min": min_tss, "max": max_tss},
            "zones": zones,
            "purpose": purpose
        }
    
    def _generate_race_alerts(self, race_calendar: Dict) -> List[Dict]:
        """Generate race-specific alerts for the alerts array."""
        alerts = []
        
        # Taper onset alert
        taper = race_calendar.get("taper_alert", {})
        if taper.get("active"):
            alerts.append({
                "metric": "race_taper",
                "value": taper.get("days_until"),
                "severity": "info",
                "threshold": "RACE_A within 8-14 days",
                "context": taper.get("message", "Taper onset detected."),
                "persistence_days": None,
                "tier": 1
            })
        
        # Race-week alerts
        rw = race_calendar.get("race_week", {})
        if rw.get("active"):
            # Daily status alert
            today_proto = rw.get("today", {})
            load = today_proto.get("load_target_tss", {})
            alerts.append({
                "metric": "race_week",
                "value": rw.get("days_until_event"),
                "severity": "info",
                "threshold": f"{rw.get('event_category')} within 7 days",
                "context": (
                    f"Race week {rw.get('current_day')} of '{rw.get('event_name')}'. "
                    f"Today: {today_proto.get('label', '?')}, "
                    f"{load.get('min', 0)}-{load.get('max', 0)} TSS. "
                    f"{today_proto.get('zones', '')}"
                ),
                "persistence_days": None,
                "tier": 1
            })
            
            # TSB projection warning
            projected = rw.get("projected_race_day_tsb")
            tsb_range = rw.get("tsb_target_range", {})
            if projected is not None and tsb_range:
                if projected < tsb_range.get("min", 0):
                    alerts.append({
                        "metric": "race_week_tsb",
                        "value": projected,
                        "severity": "warning",
                        "threshold": f"TSB target {tsb_range.get('min')}-{tsb_range.get('max')}",
                        "context": (
                            f"Projected race-day TSB {projected} is below target range "
                            f"{tsb_range.get('min')}-{tsb_range.get('max')}. "
                            f"Consider additional rest to reach target."
                        ),
                        "persistence_days": None,
                        "tier": 1
                    })
        
        return alerts
    
    def _compute_weekly_summary(self, activities: List[Dict], wellness: List[Dict]) -> Dict:
        """Compute weekly training summary from actual activity data"""
        total_tss = sum(act.get("icu_training_load", 0) for act in activities if act.get("icu_training_load"))
        total_seconds = sum(act.get("moving_time", 0) for act in activities)
        total_hours = total_seconds / 3600

        avg_hrv = None
        avg_rhr = None
        if wellness:
            hrv_values = [w.get("hrv") for w in wellness if self._is_valid_hrv(w.get("hrv"))]
            rhr_values = [w.get("restingHR") for w in wellness if w.get("restingHR")]
            avg_hrv = round(sum(hrv_values) / len(hrv_values), 1) if hrv_values else None
            avg_rhr = round(sum(rhr_values) / len(rhr_values), 1) if rhr_values else None

        return {
            "total_training_hours": round(total_hours, 2),
            "total_tss": round(total_tss, 0),
            "activities_count": len(activities),
            "avg_hrv": avg_hrv,
            "avg_resting_hr": avg_rhr
        }
    
    def _compute_activity_summary(self, activities: List[Dict], days_back: int = 7) -> Dict:
        """Compute summary by activity type with human-readable format"""
        by_type = defaultdict(lambda: {"count": 0, "seconds": 0, "tss": 0, "distance_km": 0})
        
        for act in activities:
            activity_type = act.get("type", "Unknown")
            by_type[activity_type]["count"] += 1
            
            time_seconds = act.get("moving_time", 0)
            
            by_type[activity_type]["seconds"] += time_seconds
            by_type[activity_type]["tss"] += act.get("icu_training_load", 0) or 0
            by_type[activity_type]["distance_km"] += (act.get("distance", 0) or 0) / 1000
        
        activity_breakdown = {}
        total_seconds = 0
        
        for activity_type, data in sorted(by_type.items()):
            activity_breakdown[activity_type] = {
                "duration_decimal_hours": round(data["seconds"] / 3600, 2),
                "count": data["count"],
                "tss": round(data["tss"], 0),
                "distance_km": round(data["distance_km"], 1)
            }
            total_seconds += data["seconds"]
        
        return {
            "period_description": f"Last {days_back} days of training (including today)",
            "note": "Duration calculated from API moving_time field.",
            "total_duration_decimal_hours": round(total_seconds / 3600, 2),
            "total_activities": len(activities),
            "by_activity_type": activity_breakdown
        }
    
    def publish_to_github(self, data: Dict, filepath: str = "latest.json", 
                         commit_message: str = None) -> str:
        """Publish data to GitHub repository"""
        if not self.github_token or not self.github_repo:
            raise ValueError("GitHub token and repo required for publishing")
        
        if not commit_message:
            commit_message = f"Update training data - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github+json"
        }
        
        url = f"{self.GITHUB_API_URL}/repos/{self.github_repo}/contents/{filepath}"
        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                current_file = response.json()
                current_sha = current_file["sha"]
                
                current_content = base64.b64decode(current_file["content"]).decode()
                new_content = json.dumps(data, indent=2, default=str)
                
                if current_content == new_content:
                    print("â­ï¸  No changes detected, skipping update")
                    raw_url = f"https://raw.githubusercontent.com/{self.github_repo}/main/{filepath}"
                    return raw_url
            else:
                current_sha = None
        except Exception as e:
            print(f"âš ï¸  Could not check existing file: {e}")
            current_sha = None
        
        content_json = json.dumps(data, indent=2, default=str)
        content_base64 = base64.b64encode(content_json.encode()).decode()
        
        payload = {
            "message": commit_message,
            "content": content_base64,
            "branch": "main"
        }
        
        if current_sha:
            payload["sha"] = current_sha
        
        response = requests.put(url, headers=headers, json=payload)
        response.raise_for_status()
        
        raw_url = f"https://raw.githubusercontent.com/{self.github_repo}/main/{filepath}"
        return raw_url
    
    def save_to_file(self, data: Dict, filepath: str = "latest.json"):
        """Save data to local JSON file"""
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2, default=str)
        print(f"Data saved to {filepath}")
        return filepath


def main():
    parser = argparse.ArgumentParser(description="Sync Intervals.icu data to GitHub or local file")
    parser.add_argument("--setup", action="store_true", help="Initial setup wizard")
    parser.add_argument("--athlete-id", help="Intervals.icu athlete ID")
    parser.add_argument("--intervals-key", help="Intervals.icu API key")
    parser.add_argument("--github-token", help="GitHub Personal Access Token")
    parser.add_argument("--github-repo", help="GitHub repo (format: username/repo)")
    parser.add_argument("--days", type=int, default=7, help="Days of data to export (default: 7)")
    parser.add_argument("--output", help="Save to local file instead of GitHub")
    parser.add_argument("--anonymize", action="store_true", default=True, help="Remove identifying information (default: enabled)")
    parser.add_argument("--debug", action="store_true", help="Show debug output for API fields")
    parser.add_argument("--generate-history", action="store_true", help="Force generate history.json (pulls up to 3 years)")
    
    args = parser.parse_args()
    
    if args.setup:
        print("=== Intervals.icu Sync Setup ===\n")
        athlete_id = input("Intervals.icu Athlete ID (e.g., i123456): ")
        intervals_key = input("Intervals.icu API Key: ")
        github_token = input("GitHub Personal Access Token (or press Enter to skip): ")
        github_repo = input("GitHub Repository (username/repo, or press Enter to skip): ")
        
        config = {
            "athlete_id": athlete_id,
            "intervals_key": intervals_key,
        }
        if github_token:
            config["github_token"] = github_token
        if github_repo:
            config["github_repo"] = github_repo
            
        with open(".sync_config.json", "w") as f:
            json.dump(config, f, indent=2)
        print("\nâœ… Config saved to .sync_config.json")
        print("\nUsage:")
        print("  Export locally:    python sync.py --output latest.json")
        print("  Push to GitHub:    python sync.py")
        print("  Generate history:  python sync.py --generate-history --output history.json")
        return
    
    config = {}
    if os.path.exists(".sync_config.json"):
        with open(".sync_config.json") as f:
            config = json.load(f)
    
    athlete_id = args.athlete_id or config.get("athlete_id") or os.getenv("ATHLETE_ID")
    intervals_key = args.intervals_key or config.get("intervals_key") or os.getenv("INTERVALS_KEY")
    github_token = args.github_token or config.get("github_token") or os.getenv("GITHUB_TOKEN")
    github_repo = args.github_repo or config.get("github_repo") or os.getenv("GITHUB_REPO")
    
    print(f"ðŸ“‹ Configuration:")
    print(f"   Athlete ID: {athlete_id[:5] + '...' if athlete_id else 'NOT SET'}")
    print(f"   Intervals Key: {intervals_key[:5] + '...' if intervals_key else 'NOT SET'}")
    print(f"   GitHub Repo: {github_repo or 'NOT SET'}")
    print(f"   GitHub Token: {'SET' if github_token else 'NOT SET'}")
    print(f"   Days: {args.days}")
    print(f"   Version: {IntervalsSync.VERSION}")
    
    if not athlete_id or not intervals_key:
        print("\nâŒ Error: Missing credentials.")
        print("   Run: python sync.py --setup")
        return
    
    sync = IntervalsSync(athlete_id, intervals_key, github_token, github_repo, debug=args.debug)
    
    # Manual history generation
    if args.generate_history:
        print(f"\nðŸ“Š Generating history.json (up to 3 years)...")
        history = sync.generate_history()
        dr = history.get("data_range", {})
        print(f"\nâœ… history.json generated")
        print(f"   Range: {dr.get('earliest')} â†’ {dr.get('latest')} ({dr.get('total_months')} months)")
        print(f"   FTP changes tracked: {len(history.get('ftp_timeline', []))}")
        print(f"   Data gaps found: {len(history.get('data_gaps', []))}")
        
        # Also publish to GitHub if credentials available
        if github_token and github_repo and not args.output:
            print("\nðŸ“¤ Publishing history.json to GitHub...")
            sync.publish_to_github(history, filepath="history.json",
                                   commit_message=f"Generate history.json - {datetime.now().strftime('%Y-%m-%d')}")
            print("   âœ… history.json pushed to GitHub")
        return
    
    if not args.output and (not github_token or not github_repo):
        print("\nâŒ Error: Missing GitHub credentials for push.")
        print("   Either use --output to save locally, or configure GitHub in --setup")
        return
    
    print(f"\nðŸ”„ Fetching {args.days} days of data (extended 28 days for ACWR)...")
    
    data = sync.collect_training_data(days_back=args.days, anonymize=args.anonymize)
    
    # Extract derived metrics for display
    dm = data.get("derived_metrics", {})
    alerts = data.get("alerts", [])
    
    # Common display function
    def print_summary():
        print(f"\nðŸ“Š Derived metrics:")
        print(f"   ACWR: {dm.get('acwr')} ({dm.get('acwr_interpretation')})")
        print(f"   Recovery Index: {dm.get('recovery_index')}")
        print(f"   Monotony: {dm.get('monotony')} ({dm.get('monotony_interpretation')})")
        print(f"   Strain: {dm.get('strain')}")
        print(f"   Gray Zone %: {dm.get('grey_zone_percentage')}%")
        print(f"   Quality Intensity %: {dm.get('quality_intensity_percentage')}%")
        print(f"   Polarisation: {dm.get('polarisation_index')} (target ~0.80)")
        tid = dm.get('seiler_tid_7d', {})
        tid_ps = dm.get('seiler_tid_7d_primary', {})
        print(f"   Seiler TID: {tid.get('classification')} (PI: {tid.get('polarization_index')}) â€” Z1:{tid.get('z1_pct')}% Z2:{tid.get('z2_pct')}% Z3:{tid.get('z3_pct')}%")
        if tid_ps:
            print(f"   Seiler TID ({tid_ps.get('sport')}): {tid_ps.get('classification')} (PI: {tid_ps.get('polarization_index')}) â€” Z1:{tid_ps.get('z1_pct')}% Z2:{tid_ps.get('z2_pct')}% Z3:{tid_ps.get('z3_pct')}%")
        print(f"   Consistency: {dm.get('consistency_index')}")
        print(f"   Phase: {dm.get('phase_detected')}")
        print(f"\nðŸ“ˆ Performance (from API):")
        print(f"   eFTP: {dm.get('eftp')}W")
        print(f"   W': {dm.get('w_prime_kj')}kJ")
        print(f"   P-max: {dm.get('p_max')}W")
        print(f"   VO2max: {dm.get('vo2max')}")
        bi_indoor = dm.get('benchmark_indoor', {})
        bi_outdoor = dm.get('benchmark_outdoor', {})
        print(f"   Indoor FTP:  {bi_indoor.get('current_ftp')}W â†’ Benchmark: {bi_indoor.get('benchmark_percentage') or 'N/A (need 8 weeks)'}")
        print(f"   Outdoor FTP: {bi_outdoor.get('current_ftp')}W â†’ Benchmark: {bi_outdoor.get('benchmark_percentage') or 'N/A (need 8 weeks)'}")
        
        # Display alerts
        if alerts:
            print(f"\nâš ï¸  Alerts ({len(alerts)}):")
            for alert in alerts:
                icon = "ðŸ”´" if alert["severity"] == "alarm" else "ðŸŸ¡" if alert["severity"] == "warning" else "â„¹ï¸"
                print(f"   {icon} [{alert['severity'].upper()}] {alert['metric']}: {alert['context']}")
        else:
            print(f"\nâœ… No alerts â€” green light")
        
        # Display history confidence
        history_info = data.get("history", {})
        if history_info.get("available"):
            print(f"\nðŸ“š History: available ({history_info.get('history_confidence')} confidence, {history_info.get('total_months')}mo)")
        else:
            print(f"\nðŸ“š History: not available (will auto-generate on this run)")
    
    if args.output:
        filepath = sync.save_to_file(data, args.output)
        if args.anonymize:
            print(f"   ðŸ”’ Anonymization: ENABLED")
        print(f"\nâœ… Data saved to {filepath}")
        print_summary()
        print(f"\nðŸ’¡ Tip: Paste contents to AI, or upload the file directly")
        
        # === AUTO HISTORY GENERATION (local mode) ===
        if sync.should_generate_history():
            try:
                print("\nðŸ“Š Auto-generating history.json...")
                history = sync.generate_history()
                history_path = sync.script_dir / sync.HISTORY_FILE
                with open(history_path, 'w') as f:
                    json.dump(history, f, indent=2, default=str)
                print(f"   âœ… history.json saved to {history_path}")
            except Exception as e:
                print(f"   âš ï¸ History generation failed (non-critical): {e}")
    else:
        raw_url = sync.publish_to_github(data)
        
        print(f"\nâœ… Data published to GitHub")
        if args.anonymize:
            print(f"   ðŸ”’ Anonymization: ENABLED")
        print_summary()
        print(f"\nðŸ“Š Static URL for LLMs:")
        print(f"   {raw_url}")
        print(f"\nðŸ’¬ Example prompt:")
        print(f'   "Analyze my training data from {raw_url}"')
        
        # === AUTO HISTORY GENERATION (Sundays/Mondays, first two runs after midnight) ===
        if sync.should_generate_history():
            try:
                print("\nðŸ“Š Auto-generating history.json...")
                history = sync.generate_history()
                sync.publish_to_github(history, filepath="history.json",
                                       commit_message=f"Auto-generate history.json - {datetime.now().strftime('%Y-%m-%d')}")
                print("   âœ… history.json auto-generated and pushed to GitHub")
            except Exception as e:
                print(f"   âš ï¸ History generation failed (non-critical): {e}")
        
        # === UPDATE NOTIFICATIONS ===
        try:
            print("\nðŸ”” Checking for upstream updates...")
            sync.check_upstream_updates()
        except Exception as e:
            if args.debug:
                print(f"   âš ï¸ Update check failed (non-critical): {e}")


if __name__ == "__main__":
    main()
